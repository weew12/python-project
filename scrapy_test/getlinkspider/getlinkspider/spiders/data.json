[
{"title": "初窥Scrapy", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/overview.html", "content": ["\n            \n    \n  \nScrapy 0.25 文档¶\n本文档涵盖了所有Scrapy的内容。\n\n获得帮助¶\n遇到问题了？我们来帮您！\n\n查看下 FAQ ，这里有些常见的问题的解决办法。\n寻找详细的信息？试试 索引 或者 模块索引 。\n您可以在 scrapy-users的邮件列表 中寻找内容，或者 提问问题\n在 #scrapy IRC channel 提问\n在 issue tracker 中提交Scrapy的bug\n\n\n\n第一步¶\n\n\n\n初窥Scrapy\n了解Scrapy如何祝你一臂之力。\n安装指南\n安装Scrapy。\nScrapy入门教程\n编写您的第一个Scrapy项目。\n例子\n通过把玩已存在的Scrapy项目来学习更多内容。\n\n\n\n基本概念¶\n\n\n\n命令行工具(Command line tools)\n学习用于管理Scrapy项目的命令行工具\nItems\n定义爬取的数据\nSpiders\n编写爬取网站的规则\n选择器(Selectors)\n使用XPath提取网页的数据\nScrapy终端(Scrapy shell)\n在交互环境中测试提取数据的代码\nItem Loaders\n使用爬取到的数据填充item\nItem Pipeline\n后处理(Post-process)，存储爬取的数据\nFeed exports\n以不同格式输出爬取数据到不同的存储端\nLink Extractors\n方便用于提取后续跟进链接的类。\n\n\n\n内置服务¶\n\n\n\nLogging\n了解Scrapy提供的logging功能。\n数据收集(Stats Collection)\n收集爬虫运行数据\n发送email\n当特定事件发生时发送邮件通知\nTelnet终端(Telnet Console)\n使用内置的Python终端检查运行中的crawler(爬虫)\nWeb Service\n使用web service对您的爬虫进行监控和管理\n\n\n\n解决特定问题¶\n\n\n\n常见问题(FAQ)\n常见问题的解决办法。\n调试(Debugging)Spiders\n学习如何对scrapy spider的常见问题进行debug。\nSpiders Contracts\n学习如何使用contract来测试您的spider。\n实践经验(Common Practices)\n熟悉Scrapy的一些惯例做法。\n通用爬虫(Broad Crawls)\n调整Scrapy来适应并发爬取大量网站(a lot of domains)。\n借助Firefox来爬取\n了解如何使用Firefox及其他有用的插件来爬取数据。\n使用Firebug进行爬取\n了解如何使用Firebug来爬取数据。\n调试内存溢出\n了解如何查找并让您的爬虫避免内存泄露。\n下载项目图片\n下载爬取的item中的图片。\nUbuntu 软件包\n在Ubuntu下下载最新的Scrapy。\nScrapyd\n在生产环境中部署您的Scrapy项目。\n自动限速(AutoThrottle)扩展\n根据负载(load)动态调节爬取速度。\nBenchmarking\n在您的硬件平台上测试Scrapy的性能。\nJobs: 暂停，恢复爬虫\n学习如何停止和恢复爬虫\nDjangoItem\n使用Django模型编写爬取的item\n\n\n\n扩展Scrapy¶\n\n\n\n架构概览\n了解Scrapy架构。\n下载器中间件(Downloader Middleware)\n自定义页面被请求及下载操作。\nSpider中间件(Middleware)\n自定义spider的输入与输出。\n扩展(Extensions)\n提供您自定义的功能来扩展Scrapy\n核心API\n在extension(扩展)和middleware(中间件)使用api来扩展Scrapy的功能\n\n\n\n参考¶\n\n\n\n命令行工具(Command line tools)\n学习命令行工具及所有 可用的命令 。\nRequests and Responses\n了解代表HTTP请求和回复的request,response类\nSettings\n了解如何配置Scrapy及所有 可用的设置 。\n信号(Signals)\n查看如何使用及所有可用的信号\n异常(Exceptions)\n查看所有可用的exception以及相应的意义。\nItem Exporters\n快速将您爬取到的item导出到文件中(XML, CSV等格式)\n\n\n\n其他¶\n\n\n\nRelease notes\n了解最近的Scrapy版本的修改。\nContributing to Scrapy\n了解如何为Scrapy项目做出贡献。\nVersioning and API Stability\n了解Scrapy如何命名版本以及API的稳定性。\n试验阶段特性\n了解最新的特性\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "安装指南", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/install.html", "content": ["\n            \n    \n  \n安装指南¶\n\n安装Scrapy¶\n\n注解\n请先阅读 平台安装指南.\n\n下列的安装步骤假定您已经安装好下列程序:\n\nPython 2.7\nPython Package: pip and setuptools. 现在 pip 依赖 setuptools ，如果未安装，则会自动安装 setuptools 。\nlxml. 大多数Linux发行版自带了lxml。如果缺失，请查看http://lxml.de/installation.html\nOpenSSL. 除了Windows(请查看 平台安装指南)之外的系统都已经提供。\n\n您可以使用pip来安装Scrapy(推荐使用pip来安装Python package).\n使用pip安装:\npip install Scrapy\n\n\n\n\n平台安装指南¶\n\nWindows¶\n\n从 http://python.org/download/ 上安装Python 2.7.\n\n您需要修改 PATH 环境变量，将Python的可执行程序及额外的脚本添加到系统路径中。将以下路径添加到 PATH 中:\nC:\\Python27\\;C:\\Python27\\Scripts\\;\n\n\n\n请打开命令行，并且运行以下命令来修改 PATH:\nc:\\python27\\python.exe c:\\python27\\tools\\scripts\\win_add2path.py\n\n\n关闭并重新打开命令行窗口，使之生效。运行接下来的命令来确认其输出所期望的Python版本:\npython --version\n\n\n\n从 http://sourceforge.net/projects/pywin32/ 安装 pywin32\n请确认下载符合您系统的版本(win32或者amd64)\n\n从 https://pip.pypa.io/en/latest/installing.html 安装 pip\n打开命令行窗口，确认 pip 被正确安装:\npip --version\n\n\n\n到目前为止Python 2.7 及 pip 已经可以正确运行了。接下来安装Scrapy:\npip install Scrapy\n\n\n\n\n\nUbuntu 9.10及以上版本¶\n不要 使用Ubuntu提供的 python-scrapy ，相较于最新版的Scrapy，该包版本太旧，并且运行速度也较为缓慢。\n您可以使用官方提供的 Ubuntu Packages 。该包解决了全部依赖问题，并且与最新的bug修复保持持续更新。\n\n\nArchlinux¶\n您可以依照通用的方式或者从 AUR Scrapy package 来安装Scrapy:\nyaourt -S scrapy\n\n\n\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "安装Scrapy", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/install.html#scrapy", "content": ["\n            \n    \n  \n安装指南¶\n\n安装Scrapy¶\n\n注解\n请先阅读 平台安装指南.\n\n下列的安装步骤假定您已经安装好下列程序:\n\nPython 2.7\nPython Package: pip and setuptools. 现在 pip 依赖 setuptools ，如果未安装，则会自动安装 setuptools 。\nlxml. 大多数Linux发行版自带了lxml。如果缺失，请查看http://lxml.de/installation.html\nOpenSSL. 除了Windows(请查看 平台安装指南)之外的系统都已经提供。\n\n您可以使用pip来安装Scrapy(推荐使用pip来安装Python package).\n使用pip安装:\npip install Scrapy\n\n\n\n\n平台安装指南¶\n\nWindows¶\n\n从 http://python.org/download/ 上安装Python 2.7.\n\n您需要修改 PATH 环境变量，将Python的可执行程序及额外的脚本添加到系统路径中。将以下路径添加到 PATH 中:\nC:\\Python27\\;C:\\Python27\\Scripts\\;\n\n\n\n请打开命令行，并且运行以下命令来修改 PATH:\nc:\\python27\\python.exe c:\\python27\\tools\\scripts\\win_add2path.py\n\n\n关闭并重新打开命令行窗口，使之生效。运行接下来的命令来确认其输出所期望的Python版本:\npython --version\n\n\n\n从 http://sourceforge.net/projects/pywin32/ 安装 pywin32\n请确认下载符合您系统的版本(win32或者amd64)\n\n从 https://pip.pypa.io/en/latest/installing.html 安装 pip\n打开命令行窗口，确认 pip 被正确安装:\npip --version\n\n\n\n到目前为止Python 2.7 及 pip 已经可以正确运行了。接下来安装Scrapy:\npip install Scrapy\n\n\n\n\n\nUbuntu 9.10及以上版本¶\n不要 使用Ubuntu提供的 python-scrapy ，相较于最新版的Scrapy，该包版本太旧，并且运行速度也较为缓慢。\n您可以使用官方提供的 Ubuntu Packages 。该包解决了全部依赖问题，并且与最新的bug修复保持持续更新。\n\n\nArchlinux¶\n您可以依照通用的方式或者从 AUR Scrapy package 来安装Scrapy:\nyaourt -S scrapy\n\n\n\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "平台安装指南", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/install.html#intro-install-platform-notes", "content": ["\n            \n    \n  \n安装指南¶\n\n安装Scrapy¶\n\n注解\n请先阅读 平台安装指南.\n\n下列的安装步骤假定您已经安装好下列程序:\n\nPython 2.7\nPython Package: pip and setuptools. 现在 pip 依赖 setuptools ，如果未安装，则会自动安装 setuptools 。\nlxml. 大多数Linux发行版自带了lxml。如果缺失，请查看http://lxml.de/installation.html\nOpenSSL. 除了Windows(请查看 平台安装指南)之外的系统都已经提供。\n\n您可以使用pip来安装Scrapy(推荐使用pip来安装Python package).\n使用pip安装:\npip install Scrapy\n\n\n\n\n平台安装指南¶\n\nWindows¶\n\n从 http://python.org/download/ 上安装Python 2.7.\n\n您需要修改 PATH 环境变量，将Python的可执行程序及额外的脚本添加到系统路径中。将以下路径添加到 PATH 中:\nC:\\Python27\\;C:\\Python27\\Scripts\\;\n\n\n\n请打开命令行，并且运行以下命令来修改 PATH:\nc:\\python27\\python.exe c:\\python27\\tools\\scripts\\win_add2path.py\n\n\n关闭并重新打开命令行窗口，使之生效。运行接下来的命令来确认其输出所期望的Python版本:\npython --version\n\n\n\n从 http://sourceforge.net/projects/pywin32/ 安装 pywin32\n请确认下载符合您系统的版本(win32或者amd64)\n\n从 https://pip.pypa.io/en/latest/installing.html 安装 pip\n打开命令行窗口，确认 pip 被正确安装:\npip --version\n\n\n\n到目前为止Python 2.7 及 pip 已经可以正确运行了。接下来安装Scrapy:\npip install Scrapy\n\n\n\n\n\nUbuntu 9.10及以上版本¶\n不要 使用Ubuntu提供的 python-scrapy ，相较于最新版的Scrapy，该包版本太旧，并且运行速度也较为缓慢。\n您可以使用官方提供的 Ubuntu Packages 。该包解决了全部依赖问题，并且与最新的bug修复保持持续更新。\n\n\nArchlinux¶\n您可以依照通用的方式或者从 AUR Scrapy package 来安装Scrapy:\nyaourt -S scrapy\n\n\n\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Windows", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/install.html#windows", "content": ["\n            \n    \n  \n安装指南¶\n\n安装Scrapy¶\n\n注解\n请先阅读 平台安装指南.\n\n下列的安装步骤假定您已经安装好下列程序:\n\nPython 2.7\nPython Package: pip and setuptools. 现在 pip 依赖 setuptools ，如果未安装，则会自动安装 setuptools 。\nlxml. 大多数Linux发行版自带了lxml。如果缺失，请查看http://lxml.de/installation.html\nOpenSSL. 除了Windows(请查看 平台安装指南)之外的系统都已经提供。\n\n您可以使用pip来安装Scrapy(推荐使用pip来安装Python package).\n使用pip安装:\npip install Scrapy\n\n\n\n\n平台安装指南¶\n\nWindows¶\n\n从 http://python.org/download/ 上安装Python 2.7.\n\n您需要修改 PATH 环境变量，将Python的可执行程序及额外的脚本添加到系统路径中。将以下路径添加到 PATH 中:\nC:\\Python27\\;C:\\Python27\\Scripts\\;\n\n\n\n请打开命令行，并且运行以下命令来修改 PATH:\nc:\\python27\\python.exe c:\\python27\\tools\\scripts\\win_add2path.py\n\n\n关闭并重新打开命令行窗口，使之生效。运行接下来的命令来确认其输出所期望的Python版本:\npython --version\n\n\n\n从 http://sourceforge.net/projects/pywin32/ 安装 pywin32\n请确认下载符合您系统的版本(win32或者amd64)\n\n从 https://pip.pypa.io/en/latest/installing.html 安装 pip\n打开命令行窗口，确认 pip 被正确安装:\npip --version\n\n\n\n到目前为止Python 2.7 及 pip 已经可以正确运行了。接下来安装Scrapy:\npip install Scrapy\n\n\n\n\n\nUbuntu 9.10及以上版本¶\n不要 使用Ubuntu提供的 python-scrapy ，相较于最新版的Scrapy，该包版本太旧，并且运行速度也较为缓慢。\n您可以使用官方提供的 Ubuntu Packages 。该包解决了全部依赖问题，并且与最新的bug修复保持持续更新。\n\n\nArchlinux¶\n您可以依照通用的方式或者从 AUR Scrapy package 来安装Scrapy:\nyaourt -S scrapy\n\n\n\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Ubuntu 9.10及以上版本", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/install.html#ubuntu-9-10", "content": ["\n            \n    \n  \n安装指南¶\n\n安装Scrapy¶\n\n注解\n请先阅读 平台安装指南.\n\n下列的安装步骤假定您已经安装好下列程序:\n\nPython 2.7\nPython Package: pip and setuptools. 现在 pip 依赖 setuptools ，如果未安装，则会自动安装 setuptools 。\nlxml. 大多数Linux发行版自带了lxml。如果缺失，请查看http://lxml.de/installation.html\nOpenSSL. 除了Windows(请查看 平台安装指南)之外的系统都已经提供。\n\n您可以使用pip来安装Scrapy(推荐使用pip来安装Python package).\n使用pip安装:\npip install Scrapy\n\n\n\n\n平台安装指南¶\n\nWindows¶\n\n从 http://python.org/download/ 上安装Python 2.7.\n\n您需要修改 PATH 环境变量，将Python的可执行程序及额外的脚本添加到系统路径中。将以下路径添加到 PATH 中:\nC:\\Python27\\;C:\\Python27\\Scripts\\;\n\n\n\n请打开命令行，并且运行以下命令来修改 PATH:\nc:\\python27\\python.exe c:\\python27\\tools\\scripts\\win_add2path.py\n\n\n关闭并重新打开命令行窗口，使之生效。运行接下来的命令来确认其输出所期望的Python版本:\npython --version\n\n\n\n从 http://sourceforge.net/projects/pywin32/ 安装 pywin32\n请确认下载符合您系统的版本(win32或者amd64)\n\n从 https://pip.pypa.io/en/latest/installing.html 安装 pip\n打开命令行窗口，确认 pip 被正确安装:\npip --version\n\n\n\n到目前为止Python 2.7 及 pip 已经可以正确运行了。接下来安装Scrapy:\npip install Scrapy\n\n\n\n\n\nUbuntu 9.10及以上版本¶\n不要 使用Ubuntu提供的 python-scrapy ，相较于最新版的Scrapy，该包版本太旧，并且运行速度也较为缓慢。\n您可以使用官方提供的 Ubuntu Packages 。该包解决了全部依赖问题，并且与最新的bug修复保持持续更新。\n\n\nArchlinux¶\n您可以依照通用的方式或者从 AUR Scrapy package 来安装Scrapy:\nyaourt -S scrapy\n\n\n\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Archlinux", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/install.html#archlinux", "content": ["\n            \n    \n  \n安装指南¶\n\n安装Scrapy¶\n\n注解\n请先阅读 平台安装指南.\n\n下列的安装步骤假定您已经安装好下列程序:\n\nPython 2.7\nPython Package: pip and setuptools. 现在 pip 依赖 setuptools ，如果未安装，则会自动安装 setuptools 。\nlxml. 大多数Linux发行版自带了lxml。如果缺失，请查看http://lxml.de/installation.html\nOpenSSL. 除了Windows(请查看 平台安装指南)之外的系统都已经提供。\n\n您可以使用pip来安装Scrapy(推荐使用pip来安装Python package).\n使用pip安装:\npip install Scrapy\n\n\n\n\n平台安装指南¶\n\nWindows¶\n\n从 http://python.org/download/ 上安装Python 2.7.\n\n您需要修改 PATH 环境变量，将Python的可执行程序及额外的脚本添加到系统路径中。将以下路径添加到 PATH 中:\nC:\\Python27\\;C:\\Python27\\Scripts\\;\n\n\n\n请打开命令行，并且运行以下命令来修改 PATH:\nc:\\python27\\python.exe c:\\python27\\tools\\scripts\\win_add2path.py\n\n\n关闭并重新打开命令行窗口，使之生效。运行接下来的命令来确认其输出所期望的Python版本:\npython --version\n\n\n\n从 http://sourceforge.net/projects/pywin32/ 安装 pywin32\n请确认下载符合您系统的版本(win32或者amd64)\n\n从 https://pip.pypa.io/en/latest/installing.html 安装 pip\n打开命令行窗口，确认 pip 被正确安装:\npip --version\n\n\n\n到目前为止Python 2.7 及 pip 已经可以正确运行了。接下来安装Scrapy:\npip install Scrapy\n\n\n\n\n\nUbuntu 9.10及以上版本¶\n不要 使用Ubuntu提供的 python-scrapy ，相较于最新版的Scrapy，该包版本太旧，并且运行速度也较为缓慢。\n您可以使用官方提供的 Ubuntu Packages 。该包解决了全部依赖问题，并且与最新的bug修复保持持续更新。\n\n\nArchlinux¶\n您可以依照通用的方式或者从 AUR Scrapy package 来安装Scrapy:\nyaourt -S scrapy\n\n\n\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "刚才发生了什么？", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#id4", "content": ["\n            \n    \n  \nScrapy入门教程¶\n在本篇教程中，我们假定您已经安装好Scrapy。\n如若不然，请参考 安装指南 。\n接下来以 Open Directory Project(dmoz) (dmoz)\n为例来讲述爬取。\n本篇教程中将带您完成下列任务:\n\n创建一个Scrapy项目\n定义提取的Item\n编写爬取网站的 spider 并提取 Item\n编写 Item Pipeline 来存储提取到的Item(即数据)\n\nScrapy由 Python 编写。如果您刚接触并且好奇这门语言的特性以及Scrapy的详情，\n对于已经熟悉其他语言并且想快速学习Python的编程老手，\n我们推荐 Learn Python The Hard Way ，\n对于想从Python开始学习的编程新手，\n非程序员的Python学习资料列表 将是您的选择。\n\n创建项目¶\n在开始爬取之前，您必须创建一个新的Scrapy项目。\n进入您打算存储代码的目录中，运行下列命令:\nscrapy startproject tutorial\n\n\n该命令将会创建包含下列内容的 tutorial 目录:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\n\n这些文件分别是:\n\nscrapy.cfg: 项目的配置文件\ntutorial/: 该项目的python模块。之后您将在此加入代码。\ntutorial/items.py: 项目中的item文件.\ntutorial/pipelines.py: 项目中的pipelines文件.\ntutorial/settings.py: 项目的设置文件.\ntutorial/spiders/: 放置spider代码的目录.\n\n\n\n定义Item¶\nItem 是保存爬取到的数据的容器；其使用方法和python字典类似，\n并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类，\n并且定义类型为 scrapy.Field 的类属性来定义一个Item。\n(如果不了解ORM, 不用担心，您会发现这个步骤非常简单)\n首先根据需要从dmoz.org获取到的数据对item进行建模。\n我们需要从dmoz中获取名字，url，以及网站的描述。\n对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件:\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\n\n一开始这看起来可能有点复杂，但是通过定义item，\n您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。\n\n\n编写第一个爬虫(Spider)¶\nSpider是用户编写用于从单个网站(或者一些网站)爬取数据的类。\n其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容，\n提取生成 item 的方法。\n为了创建一个Spider，您必须继承 scrapy.Spider 类，\n且定义以下三个属性:\n\nname: 用于区别Spider。\n该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。\nstart_urls: 包含了Spider在启动时进行爬取的url列表。\n因此，第一个被获取到的页面将是其中之一。\n后续的URL则从初始的URL获取到的数据中提取。\nparse() 是spider的一个方法。\n被调用时，每个初始URL完成下载后生成的 Response\n对象将会作为唯一的参数传递给该函数。\n该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。\n\n以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中:\nimport scrapy\n\nclass DmozSpider(scrapy.spiders.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n\n\n\n爬取¶\n进入项目的根目录，执行下列命令启动spider:\nscrapy crawl dmoz\n\n\ncrawl dmoz 启动用于爬取 dmoz.org 的spider，您将得到类似的输出:\n2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)\n2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...\n2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened\n2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)\n\n\n查看包含 [dmoz] 的输出，可以看到输出的log中包含定义在 start_urls 的初始URL，并且与spider中是一一对应的。在log中可以看到其没有指向其他页面( (referer:None) )。\n除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含url所对应的内容的文件被创建了: Book , Resources 。\n\n刚才发生了什么？¶\nScrapy为Spider的 start_urls 属性中的每个URL创建了 scrapy.Request 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。\nRequest对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。\n\n\n\n提取Item¶\n\nSelectors选择器简介¶\n从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制:\nScrapy Selectors 。\n关于selector和其他提取机制的信息请参考 Selector文档 。\n这里给出XPath表达式的例子及对应的含义:\n\n/html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素\n/html/head/title/text(): 选择上面提到的 <title> 元素的文字\n//td: 选择所有的 <td> 元素\n//div[@class=\"mine\"]: 选择所有具有 class=\"mine\" 属性的 div 元素\n\n上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。\n如果您想了解的更多，我们推荐 这篇XPath教程 。\n为了配合XPath，Scrapy除了提供了 Selector\n之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。\nSelector有四个基本的方法(点击相应的方法可以看到详细的API文档):\n\nxpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。\ncss(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.\nextract(): 序列化该节点为unicode字符串并返回list。\nre(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。\n\n\n\n在Shell中尝试Selector选择器¶\n为了介绍Selector的使用方法，接下来我们将要使用内置的 Scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。\n您需要进入项目的根目录，执行下列命令来启动shell:\nscrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\n\n\n\n注解\n当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 & 字符)会导致Scrapy运行失败。\n\nshell的输出类似:\n[ ... Scrapy log here ... ]\n\n2015-01-07 22:01:53+0800 [domz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x02CE2530>\n[s]   item       {}\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   sel        <Selector xpath=None data=u'<html lang=\"en\">\\r\\n<head>\\r\\n<meta http-equ'>\n[s]   settings   <CrawlerSettings module=<module 'tutorial.settings' from 'tutorial\\settings.pyc'>>\n[s]   spider     <DomzSpider 'domz' at 0x302e350>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n>>>\n\n\n当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。\n更为重要的是，当输入 response.selector 时，\n您将获取到一个可以用于查询返回数据的selector(选择器)，\n以及映射到 response.selector.xpath() 、 response.selector.css() 的\n快捷方法(shortcut): response.xpath() 和 response.css() 。\n同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。\n让我们来试试:\nIn [1]: sel.xpath('//title')\nOut[1]: [<Selector xpath='//title' data=u'<title>Open Directory - Computers: Progr'>]\n\nIn [2]: sel.xpath('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: sel.xpath('//title/text()')\nOut[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]\n\nIn [4]: sel.xpath('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: sel.xpath('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\n\n提取数据¶\n现在，我们来尝试从这些页面中提取些有用的数据。\n您可以在终端中输入 response.body 来观察HTML源码并确定合适的XPath表达式。不过，这任务非常无聊且不易。您可以考虑使用Firefox的Firebug扩展来使得工作更为轻松。详情请参考 使用Firebug进行爬取 和 借助Firefox来爬取 。\n在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。\n我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:\nsel.xpath('//ul/li')\n\n\n网站的描述:\nsel.xpath('//ul/li/text()').extract()\n\n\n网站的标题:\nsel.xpath('//ul/li/a/text()').extract()\n\n\n以及网站的链接:\nsel.xpath('//ul/li/a/@href').extract()\n\n\n之前提到过，每个 .xpath() 调用返回selector组成的list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性:\nfor sel in response.xpath('//ul/li'):\n    title = sel.xpath('a/text()').extract()\n    link = sel.xpath('a/@href').extract()\n    desc = sel.xpath('text()').extract()\n    print title, link, desc\n\n\n\n注解\n关于嵌套selctor的更多详细信息，请参考 嵌套选择器(selectors) 以及 选择器(Selectors) 文档中的 使用相对XPaths 部分。\n\n在我们的spider中加入这段代码:\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            title = sel.xpath('a/text()').extract()\n            link = sel.xpath('a/@href').extract()\n            desc = sel.xpath('text()').extract()\n            print title, link, desc\n\n\n现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出:\nscrapy crawl dmoz\n\n\n\n\n\n使用item¶\nItem 对象是自定义的python字典。\n您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\n一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:\nimport scrapy\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\n\n\n\n注解\n您可以在 dirbot 项目中找到一个具有完整功能的spider。该项目可以通过 https://github.com/scrapy/dirbot 找到。\n\n现在对dmoz.org进行爬取将会产生 DmozItem 对象:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\n\n保存爬取到的数据¶\n最简单存储爬取的数据的方式是使用 Feed exports:\nscrapy crawl dmoz -o items.json\n\n\n该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。\n在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。\n如果需要对爬取到的item做更多更为复杂的操作，您可以编写\nItem Pipeline 。\n类似于我们在创建项目时对Item做的，用于您编写自己的\ntutorial/pipelines.py 也被创建。\n不过如果您仅仅想要保存item，您不需要实现任何的pipeline。\n\n\n下一步¶\n本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及。请查看 初窥Scrapy 章节中的 还有什么？ 部分,大致浏览大部分重要的特性。\n接着，我们推荐您把玩一个例子(查看 例子)，而后继续阅读 基本概念 。\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "提取Item", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#id5", "content": ["\n            \n    \n  \nScrapy入门教程¶\n在本篇教程中，我们假定您已经安装好Scrapy。\n如若不然，请参考 安装指南 。\n接下来以 Open Directory Project(dmoz) (dmoz)\n为例来讲述爬取。\n本篇教程中将带您完成下列任务:\n\n创建一个Scrapy项目\n定义提取的Item\n编写爬取网站的 spider 并提取 Item\n编写 Item Pipeline 来存储提取到的Item(即数据)\n\nScrapy由 Python 编写。如果您刚接触并且好奇这门语言的特性以及Scrapy的详情，\n对于已经熟悉其他语言并且想快速学习Python的编程老手，\n我们推荐 Learn Python The Hard Way ，\n对于想从Python开始学习的编程新手，\n非程序员的Python学习资料列表 将是您的选择。\n\n创建项目¶\n在开始爬取之前，您必须创建一个新的Scrapy项目。\n进入您打算存储代码的目录中，运行下列命令:\nscrapy startproject tutorial\n\n\n该命令将会创建包含下列内容的 tutorial 目录:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\n\n这些文件分别是:\n\nscrapy.cfg: 项目的配置文件\ntutorial/: 该项目的python模块。之后您将在此加入代码。\ntutorial/items.py: 项目中的item文件.\ntutorial/pipelines.py: 项目中的pipelines文件.\ntutorial/settings.py: 项目的设置文件.\ntutorial/spiders/: 放置spider代码的目录.\n\n\n\n定义Item¶\nItem 是保存爬取到的数据的容器；其使用方法和python字典类似，\n并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类，\n并且定义类型为 scrapy.Field 的类属性来定义一个Item。\n(如果不了解ORM, 不用担心，您会发现这个步骤非常简单)\n首先根据需要从dmoz.org获取到的数据对item进行建模。\n我们需要从dmoz中获取名字，url，以及网站的描述。\n对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件:\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\n\n一开始这看起来可能有点复杂，但是通过定义item，\n您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。\n\n\n编写第一个爬虫(Spider)¶\nSpider是用户编写用于从单个网站(或者一些网站)爬取数据的类。\n其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容，\n提取生成 item 的方法。\n为了创建一个Spider，您必须继承 scrapy.Spider 类，\n且定义以下三个属性:\n\nname: 用于区别Spider。\n该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。\nstart_urls: 包含了Spider在启动时进行爬取的url列表。\n因此，第一个被获取到的页面将是其中之一。\n后续的URL则从初始的URL获取到的数据中提取。\nparse() 是spider的一个方法。\n被调用时，每个初始URL完成下载后生成的 Response\n对象将会作为唯一的参数传递给该函数。\n该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。\n\n以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中:\nimport scrapy\n\nclass DmozSpider(scrapy.spiders.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n\n\n\n爬取¶\n进入项目的根目录，执行下列命令启动spider:\nscrapy crawl dmoz\n\n\ncrawl dmoz 启动用于爬取 dmoz.org 的spider，您将得到类似的输出:\n2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)\n2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...\n2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened\n2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)\n\n\n查看包含 [dmoz] 的输出，可以看到输出的log中包含定义在 start_urls 的初始URL，并且与spider中是一一对应的。在log中可以看到其没有指向其他页面( (referer:None) )。\n除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含url所对应的内容的文件被创建了: Book , Resources 。\n\n刚才发生了什么？¶\nScrapy为Spider的 start_urls 属性中的每个URL创建了 scrapy.Request 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。\nRequest对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。\n\n\n\n提取Item¶\n\nSelectors选择器简介¶\n从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制:\nScrapy Selectors 。\n关于selector和其他提取机制的信息请参考 Selector文档 。\n这里给出XPath表达式的例子及对应的含义:\n\n/html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素\n/html/head/title/text(): 选择上面提到的 <title> 元素的文字\n//td: 选择所有的 <td> 元素\n//div[@class=\"mine\"]: 选择所有具有 class=\"mine\" 属性的 div 元素\n\n上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。\n如果您想了解的更多，我们推荐 这篇XPath教程 。\n为了配合XPath，Scrapy除了提供了 Selector\n之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。\nSelector有四个基本的方法(点击相应的方法可以看到详细的API文档):\n\nxpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。\ncss(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.\nextract(): 序列化该节点为unicode字符串并返回list。\nre(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。\n\n\n\n在Shell中尝试Selector选择器¶\n为了介绍Selector的使用方法，接下来我们将要使用内置的 Scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。\n您需要进入项目的根目录，执行下列命令来启动shell:\nscrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\n\n\n\n注解\n当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 & 字符)会导致Scrapy运行失败。\n\nshell的输出类似:\n[ ... Scrapy log here ... ]\n\n2015-01-07 22:01:53+0800 [domz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x02CE2530>\n[s]   item       {}\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   sel        <Selector xpath=None data=u'<html lang=\"en\">\\r\\n<head>\\r\\n<meta http-equ'>\n[s]   settings   <CrawlerSettings module=<module 'tutorial.settings' from 'tutorial\\settings.pyc'>>\n[s]   spider     <DomzSpider 'domz' at 0x302e350>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n>>>\n\n\n当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。\n更为重要的是，当输入 response.selector 时，\n您将获取到一个可以用于查询返回数据的selector(选择器)，\n以及映射到 response.selector.xpath() 、 response.selector.css() 的\n快捷方法(shortcut): response.xpath() 和 response.css() 。\n同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。\n让我们来试试:\nIn [1]: sel.xpath('//title')\nOut[1]: [<Selector xpath='//title' data=u'<title>Open Directory - Computers: Progr'>]\n\nIn [2]: sel.xpath('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: sel.xpath('//title/text()')\nOut[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]\n\nIn [4]: sel.xpath('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: sel.xpath('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\n\n提取数据¶\n现在，我们来尝试从这些页面中提取些有用的数据。\n您可以在终端中输入 response.body 来观察HTML源码并确定合适的XPath表达式。不过，这任务非常无聊且不易。您可以考虑使用Firefox的Firebug扩展来使得工作更为轻松。详情请参考 使用Firebug进行爬取 和 借助Firefox来爬取 。\n在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。\n我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:\nsel.xpath('//ul/li')\n\n\n网站的描述:\nsel.xpath('//ul/li/text()').extract()\n\n\n网站的标题:\nsel.xpath('//ul/li/a/text()').extract()\n\n\n以及网站的链接:\nsel.xpath('//ul/li/a/@href').extract()\n\n\n之前提到过，每个 .xpath() 调用返回selector组成的list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性:\nfor sel in response.xpath('//ul/li'):\n    title = sel.xpath('a/text()').extract()\n    link = sel.xpath('a/@href').extract()\n    desc = sel.xpath('text()').extract()\n    print title, link, desc\n\n\n\n注解\n关于嵌套selctor的更多详细信息，请参考 嵌套选择器(selectors) 以及 选择器(Selectors) 文档中的 使用相对XPaths 部分。\n\n在我们的spider中加入这段代码:\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            title = sel.xpath('a/text()').extract()\n            link = sel.xpath('a/@href').extract()\n            desc = sel.xpath('text()').extract()\n            print title, link, desc\n\n\n现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出:\nscrapy crawl dmoz\n\n\n\n\n\n使用item¶\nItem 对象是自定义的python字典。\n您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\n一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:\nimport scrapy\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\n\n\n\n注解\n您可以在 dirbot 项目中找到一个具有完整功能的spider。该项目可以通过 https://github.com/scrapy/dirbot 找到。\n\n现在对dmoz.org进行爬取将会产生 DmozItem 对象:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\n\n保存爬取到的数据¶\n最简单存储爬取的数据的方式是使用 Feed exports:\nscrapy crawl dmoz -o items.json\n\n\n该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。\n在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。\n如果需要对爬取到的item做更多更为复杂的操作，您可以编写\nItem Pipeline 。\n类似于我们在创建项目时对Item做的，用于您编写自己的\ntutorial/pipelines.py 也被创建。\n不过如果您仅仅想要保存item，您不需要实现任何的pipeline。\n\n\n下一步¶\n本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及。请查看 初窥Scrapy 章节中的 还有什么？ 部分,大致浏览大部分重要的特性。\n接着，我们推荐您把玩一个例子(查看 例子)，而后继续阅读 基本概念 。\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Selectors选择器简介", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#selectors", "content": ["\n            \n    \n  \nScrapy入门教程¶\n在本篇教程中，我们假定您已经安装好Scrapy。\n如若不然，请参考 安装指南 。\n接下来以 Open Directory Project(dmoz) (dmoz)\n为例来讲述爬取。\n本篇教程中将带您完成下列任务:\n\n创建一个Scrapy项目\n定义提取的Item\n编写爬取网站的 spider 并提取 Item\n编写 Item Pipeline 来存储提取到的Item(即数据)\n\nScrapy由 Python 编写。如果您刚接触并且好奇这门语言的特性以及Scrapy的详情，\n对于已经熟悉其他语言并且想快速学习Python的编程老手，\n我们推荐 Learn Python The Hard Way ，\n对于想从Python开始学习的编程新手，\n非程序员的Python学习资料列表 将是您的选择。\n\n创建项目¶\n在开始爬取之前，您必须创建一个新的Scrapy项目。\n进入您打算存储代码的目录中，运行下列命令:\nscrapy startproject tutorial\n\n\n该命令将会创建包含下列内容的 tutorial 目录:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\n\n这些文件分别是:\n\nscrapy.cfg: 项目的配置文件\ntutorial/: 该项目的python模块。之后您将在此加入代码。\ntutorial/items.py: 项目中的item文件.\ntutorial/pipelines.py: 项目中的pipelines文件.\ntutorial/settings.py: 项目的设置文件.\ntutorial/spiders/: 放置spider代码的目录.\n\n\n\n定义Item¶\nItem 是保存爬取到的数据的容器；其使用方法和python字典类似，\n并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类，\n并且定义类型为 scrapy.Field 的类属性来定义一个Item。\n(如果不了解ORM, 不用担心，您会发现这个步骤非常简单)\n首先根据需要从dmoz.org获取到的数据对item进行建模。\n我们需要从dmoz中获取名字，url，以及网站的描述。\n对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件:\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\n\n一开始这看起来可能有点复杂，但是通过定义item，\n您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。\n\n\n编写第一个爬虫(Spider)¶\nSpider是用户编写用于从单个网站(或者一些网站)爬取数据的类。\n其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容，\n提取生成 item 的方法。\n为了创建一个Spider，您必须继承 scrapy.Spider 类，\n且定义以下三个属性:\n\nname: 用于区别Spider。\n该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。\nstart_urls: 包含了Spider在启动时进行爬取的url列表。\n因此，第一个被获取到的页面将是其中之一。\n后续的URL则从初始的URL获取到的数据中提取。\nparse() 是spider的一个方法。\n被调用时，每个初始URL完成下载后生成的 Response\n对象将会作为唯一的参数传递给该函数。\n该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。\n\n以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中:\nimport scrapy\n\nclass DmozSpider(scrapy.spiders.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n\n\n\n爬取¶\n进入项目的根目录，执行下列命令启动spider:\nscrapy crawl dmoz\n\n\ncrawl dmoz 启动用于爬取 dmoz.org 的spider，您将得到类似的输出:\n2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)\n2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...\n2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened\n2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)\n\n\n查看包含 [dmoz] 的输出，可以看到输出的log中包含定义在 start_urls 的初始URL，并且与spider中是一一对应的。在log中可以看到其没有指向其他页面( (referer:None) )。\n除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含url所对应的内容的文件被创建了: Book , Resources 。\n\n刚才发生了什么？¶\nScrapy为Spider的 start_urls 属性中的每个URL创建了 scrapy.Request 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。\nRequest对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。\n\n\n\n提取Item¶\n\nSelectors选择器简介¶\n从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制:\nScrapy Selectors 。\n关于selector和其他提取机制的信息请参考 Selector文档 。\n这里给出XPath表达式的例子及对应的含义:\n\n/html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素\n/html/head/title/text(): 选择上面提到的 <title> 元素的文字\n//td: 选择所有的 <td> 元素\n//div[@class=\"mine\"]: 选择所有具有 class=\"mine\" 属性的 div 元素\n\n上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。\n如果您想了解的更多，我们推荐 这篇XPath教程 。\n为了配合XPath，Scrapy除了提供了 Selector\n之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。\nSelector有四个基本的方法(点击相应的方法可以看到详细的API文档):\n\nxpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。\ncss(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.\nextract(): 序列化该节点为unicode字符串并返回list。\nre(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。\n\n\n\n在Shell中尝试Selector选择器¶\n为了介绍Selector的使用方法，接下来我们将要使用内置的 Scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。\n您需要进入项目的根目录，执行下列命令来启动shell:\nscrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\n\n\n\n注解\n当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 & 字符)会导致Scrapy运行失败。\n\nshell的输出类似:\n[ ... Scrapy log here ... ]\n\n2015-01-07 22:01:53+0800 [domz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x02CE2530>\n[s]   item       {}\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   sel        <Selector xpath=None data=u'<html lang=\"en\">\\r\\n<head>\\r\\n<meta http-equ'>\n[s]   settings   <CrawlerSettings module=<module 'tutorial.settings' from 'tutorial\\settings.pyc'>>\n[s]   spider     <DomzSpider 'domz' at 0x302e350>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n>>>\n\n\n当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。\n更为重要的是，当输入 response.selector 时，\n您将获取到一个可以用于查询返回数据的selector(选择器)，\n以及映射到 response.selector.xpath() 、 response.selector.css() 的\n快捷方法(shortcut): response.xpath() 和 response.css() 。\n同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。\n让我们来试试:\nIn [1]: sel.xpath('//title')\nOut[1]: [<Selector xpath='//title' data=u'<title>Open Directory - Computers: Progr'>]\n\nIn [2]: sel.xpath('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: sel.xpath('//title/text()')\nOut[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]\n\nIn [4]: sel.xpath('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: sel.xpath('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\n\n提取数据¶\n现在，我们来尝试从这些页面中提取些有用的数据。\n您可以在终端中输入 response.body 来观察HTML源码并确定合适的XPath表达式。不过，这任务非常无聊且不易。您可以考虑使用Firefox的Firebug扩展来使得工作更为轻松。详情请参考 使用Firebug进行爬取 和 借助Firefox来爬取 。\n在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。\n我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:\nsel.xpath('//ul/li')\n\n\n网站的描述:\nsel.xpath('//ul/li/text()').extract()\n\n\n网站的标题:\nsel.xpath('//ul/li/a/text()').extract()\n\n\n以及网站的链接:\nsel.xpath('//ul/li/a/@href').extract()\n\n\n之前提到过，每个 .xpath() 调用返回selector组成的list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性:\nfor sel in response.xpath('//ul/li'):\n    title = sel.xpath('a/text()').extract()\n    link = sel.xpath('a/@href').extract()\n    desc = sel.xpath('text()').extract()\n    print title, link, desc\n\n\n\n注解\n关于嵌套selctor的更多详细信息，请参考 嵌套选择器(selectors) 以及 选择器(Selectors) 文档中的 使用相对XPaths 部分。\n\n在我们的spider中加入这段代码:\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            title = sel.xpath('a/text()').extract()\n            link = sel.xpath('a/@href').extract()\n            desc = sel.xpath('text()').extract()\n            print title, link, desc\n\n\n现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出:\nscrapy crawl dmoz\n\n\n\n\n\n使用item¶\nItem 对象是自定义的python字典。\n您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\n一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:\nimport scrapy\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\n\n\n\n注解\n您可以在 dirbot 项目中找到一个具有完整功能的spider。该项目可以通过 https://github.com/scrapy/dirbot 找到。\n\n现在对dmoz.org进行爬取将会产生 DmozItem 对象:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\n\n保存爬取到的数据¶\n最简单存储爬取的数据的方式是使用 Feed exports:\nscrapy crawl dmoz -o items.json\n\n\n该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。\n在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。\n如果需要对爬取到的item做更多更为复杂的操作，您可以编写\nItem Pipeline 。\n类似于我们在创建项目时对Item做的，用于您编写自己的\ntutorial/pipelines.py 也被创建。\n不过如果您仅仅想要保存item，您不需要实现任何的pipeline。\n\n\n下一步¶\n本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及。请查看 初窥Scrapy 章节中的 还有什么？ 部分,大致浏览大部分重要的特性。\n接着，我们推荐您把玩一个例子(查看 例子)，而后继续阅读 基本概念 。\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "在Shell中尝试Selector选择器", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#shellselector", "content": ["\n            \n    \n  \nScrapy入门教程¶\n在本篇教程中，我们假定您已经安装好Scrapy。\n如若不然，请参考 安装指南 。\n接下来以 Open Directory Project(dmoz) (dmoz)\n为例来讲述爬取。\n本篇教程中将带您完成下列任务:\n\n创建一个Scrapy项目\n定义提取的Item\n编写爬取网站的 spider 并提取 Item\n编写 Item Pipeline 来存储提取到的Item(即数据)\n\nScrapy由 Python 编写。如果您刚接触并且好奇这门语言的特性以及Scrapy的详情，\n对于已经熟悉其他语言并且想快速学习Python的编程老手，\n我们推荐 Learn Python The Hard Way ，\n对于想从Python开始学习的编程新手，\n非程序员的Python学习资料列表 将是您的选择。\n\n创建项目¶\n在开始爬取之前，您必须创建一个新的Scrapy项目。\n进入您打算存储代码的目录中，运行下列命令:\nscrapy startproject tutorial\n\n\n该命令将会创建包含下列内容的 tutorial 目录:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\n\n这些文件分别是:\n\nscrapy.cfg: 项目的配置文件\ntutorial/: 该项目的python模块。之后您将在此加入代码。\ntutorial/items.py: 项目中的item文件.\ntutorial/pipelines.py: 项目中的pipelines文件.\ntutorial/settings.py: 项目的设置文件.\ntutorial/spiders/: 放置spider代码的目录.\n\n\n\n定义Item¶\nItem 是保存爬取到的数据的容器；其使用方法和python字典类似，\n并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类，\n并且定义类型为 scrapy.Field 的类属性来定义一个Item。\n(如果不了解ORM, 不用担心，您会发现这个步骤非常简单)\n首先根据需要从dmoz.org获取到的数据对item进行建模。\n我们需要从dmoz中获取名字，url，以及网站的描述。\n对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件:\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\n\n一开始这看起来可能有点复杂，但是通过定义item，\n您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。\n\n\n编写第一个爬虫(Spider)¶\nSpider是用户编写用于从单个网站(或者一些网站)爬取数据的类。\n其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容，\n提取生成 item 的方法。\n为了创建一个Spider，您必须继承 scrapy.Spider 类，\n且定义以下三个属性:\n\nname: 用于区别Spider。\n该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。\nstart_urls: 包含了Spider在启动时进行爬取的url列表。\n因此，第一个被获取到的页面将是其中之一。\n后续的URL则从初始的URL获取到的数据中提取。\nparse() 是spider的一个方法。\n被调用时，每个初始URL完成下载后生成的 Response\n对象将会作为唯一的参数传递给该函数。\n该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。\n\n以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中:\nimport scrapy\n\nclass DmozSpider(scrapy.spiders.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n\n\n\n爬取¶\n进入项目的根目录，执行下列命令启动spider:\nscrapy crawl dmoz\n\n\ncrawl dmoz 启动用于爬取 dmoz.org 的spider，您将得到类似的输出:\n2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)\n2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...\n2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened\n2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)\n\n\n查看包含 [dmoz] 的输出，可以看到输出的log中包含定义在 start_urls 的初始URL，并且与spider中是一一对应的。在log中可以看到其没有指向其他页面( (referer:None) )。\n除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含url所对应的内容的文件被创建了: Book , Resources 。\n\n刚才发生了什么？¶\nScrapy为Spider的 start_urls 属性中的每个URL创建了 scrapy.Request 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。\nRequest对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。\n\n\n\n提取Item¶\n\nSelectors选择器简介¶\n从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制:\nScrapy Selectors 。\n关于selector和其他提取机制的信息请参考 Selector文档 。\n这里给出XPath表达式的例子及对应的含义:\n\n/html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素\n/html/head/title/text(): 选择上面提到的 <title> 元素的文字\n//td: 选择所有的 <td> 元素\n//div[@class=\"mine\"]: 选择所有具有 class=\"mine\" 属性的 div 元素\n\n上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。\n如果您想了解的更多，我们推荐 这篇XPath教程 。\n为了配合XPath，Scrapy除了提供了 Selector\n之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。\nSelector有四个基本的方法(点击相应的方法可以看到详细的API文档):\n\nxpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。\ncss(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.\nextract(): 序列化该节点为unicode字符串并返回list。\nre(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。\n\n\n\n在Shell中尝试Selector选择器¶\n为了介绍Selector的使用方法，接下来我们将要使用内置的 Scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。\n您需要进入项目的根目录，执行下列命令来启动shell:\nscrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\n\n\n\n注解\n当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 & 字符)会导致Scrapy运行失败。\n\nshell的输出类似:\n[ ... Scrapy log here ... ]\n\n2015-01-07 22:01:53+0800 [domz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x02CE2530>\n[s]   item       {}\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   sel        <Selector xpath=None data=u'<html lang=\"en\">\\r\\n<head>\\r\\n<meta http-equ'>\n[s]   settings   <CrawlerSettings module=<module 'tutorial.settings' from 'tutorial\\settings.pyc'>>\n[s]   spider     <DomzSpider 'domz' at 0x302e350>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n>>>\n\n\n当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。\n更为重要的是，当输入 response.selector 时，\n您将获取到一个可以用于查询返回数据的selector(选择器)，\n以及映射到 response.selector.xpath() 、 response.selector.css() 的\n快捷方法(shortcut): response.xpath() 和 response.css() 。\n同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。\n让我们来试试:\nIn [1]: sel.xpath('//title')\nOut[1]: [<Selector xpath='//title' data=u'<title>Open Directory - Computers: Progr'>]\n\nIn [2]: sel.xpath('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: sel.xpath('//title/text()')\nOut[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]\n\nIn [4]: sel.xpath('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: sel.xpath('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\n\n提取数据¶\n现在，我们来尝试从这些页面中提取些有用的数据。\n您可以在终端中输入 response.body 来观察HTML源码并确定合适的XPath表达式。不过，这任务非常无聊且不易。您可以考虑使用Firefox的Firebug扩展来使得工作更为轻松。详情请参考 使用Firebug进行爬取 和 借助Firefox来爬取 。\n在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。\n我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:\nsel.xpath('//ul/li')\n\n\n网站的描述:\nsel.xpath('//ul/li/text()').extract()\n\n\n网站的标题:\nsel.xpath('//ul/li/a/text()').extract()\n\n\n以及网站的链接:\nsel.xpath('//ul/li/a/@href').extract()\n\n\n之前提到过，每个 .xpath() 调用返回selector组成的list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性:\nfor sel in response.xpath('//ul/li'):\n    title = sel.xpath('a/text()').extract()\n    link = sel.xpath('a/@href').extract()\n    desc = sel.xpath('text()').extract()\n    print title, link, desc\n\n\n\n注解\n关于嵌套selctor的更多详细信息，请参考 嵌套选择器(selectors) 以及 选择器(Selectors) 文档中的 使用相对XPaths 部分。\n\n在我们的spider中加入这段代码:\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            title = sel.xpath('a/text()').extract()\n            link = sel.xpath('a/@href').extract()\n            desc = sel.xpath('text()').extract()\n            print title, link, desc\n\n\n现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出:\nscrapy crawl dmoz\n\n\n\n\n\n使用item¶\nItem 对象是自定义的python字典。\n您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\n一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:\nimport scrapy\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\n\n\n\n注解\n您可以在 dirbot 项目中找到一个具有完整功能的spider。该项目可以通过 https://github.com/scrapy/dirbot 找到。\n\n现在对dmoz.org进行爬取将会产生 DmozItem 对象:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\n\n保存爬取到的数据¶\n最简单存储爬取的数据的方式是使用 Feed exports:\nscrapy crawl dmoz -o items.json\n\n\n该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。\n在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。\n如果需要对爬取到的item做更多更为复杂的操作，您可以编写\nItem Pipeline 。\n类似于我们在创建项目时对Item做的，用于您编写自己的\ntutorial/pipelines.py 也被创建。\n不过如果您仅仅想要保存item，您不需要实现任何的pipeline。\n\n\n下一步¶\n本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及。请查看 初窥Scrapy 章节中的 还有什么？ 部分,大致浏览大部分重要的特性。\n接着，我们推荐您把玩一个例子(查看 例子)，而后继续阅读 基本概念 。\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "提取数据", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#id7", "content": ["\n            \n    \n  \nScrapy入门教程¶\n在本篇教程中，我们假定您已经安装好Scrapy。\n如若不然，请参考 安装指南 。\n接下来以 Open Directory Project(dmoz) (dmoz)\n为例来讲述爬取。\n本篇教程中将带您完成下列任务:\n\n创建一个Scrapy项目\n定义提取的Item\n编写爬取网站的 spider 并提取 Item\n编写 Item Pipeline 来存储提取到的Item(即数据)\n\nScrapy由 Python 编写。如果您刚接触并且好奇这门语言的特性以及Scrapy的详情，\n对于已经熟悉其他语言并且想快速学习Python的编程老手，\n我们推荐 Learn Python The Hard Way ，\n对于想从Python开始学习的编程新手，\n非程序员的Python学习资料列表 将是您的选择。\n\n创建项目¶\n在开始爬取之前，您必须创建一个新的Scrapy项目。\n进入您打算存储代码的目录中，运行下列命令:\nscrapy startproject tutorial\n\n\n该命令将会创建包含下列内容的 tutorial 目录:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\n\n这些文件分别是:\n\nscrapy.cfg: 项目的配置文件\ntutorial/: 该项目的python模块。之后您将在此加入代码。\ntutorial/items.py: 项目中的item文件.\ntutorial/pipelines.py: 项目中的pipelines文件.\ntutorial/settings.py: 项目的设置文件.\ntutorial/spiders/: 放置spider代码的目录.\n\n\n\n定义Item¶\nItem 是保存爬取到的数据的容器；其使用方法和python字典类似，\n并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类，\n并且定义类型为 scrapy.Field 的类属性来定义一个Item。\n(如果不了解ORM, 不用担心，您会发现这个步骤非常简单)\n首先根据需要从dmoz.org获取到的数据对item进行建模。\n我们需要从dmoz中获取名字，url，以及网站的描述。\n对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件:\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\n\n一开始这看起来可能有点复杂，但是通过定义item，\n您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。\n\n\n编写第一个爬虫(Spider)¶\nSpider是用户编写用于从单个网站(或者一些网站)爬取数据的类。\n其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容，\n提取生成 item 的方法。\n为了创建一个Spider，您必须继承 scrapy.Spider 类，\n且定义以下三个属性:\n\nname: 用于区别Spider。\n该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。\nstart_urls: 包含了Spider在启动时进行爬取的url列表。\n因此，第一个被获取到的页面将是其中之一。\n后续的URL则从初始的URL获取到的数据中提取。\nparse() 是spider的一个方法。\n被调用时，每个初始URL完成下载后生成的 Response\n对象将会作为唯一的参数传递给该函数。\n该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。\n\n以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中:\nimport scrapy\n\nclass DmozSpider(scrapy.spiders.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n\n\n\n爬取¶\n进入项目的根目录，执行下列命令启动spider:\nscrapy crawl dmoz\n\n\ncrawl dmoz 启动用于爬取 dmoz.org 的spider，您将得到类似的输出:\n2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)\n2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...\n2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened\n2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)\n\n\n查看包含 [dmoz] 的输出，可以看到输出的log中包含定义在 start_urls 的初始URL，并且与spider中是一一对应的。在log中可以看到其没有指向其他页面( (referer:None) )。\n除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含url所对应的内容的文件被创建了: Book , Resources 。\n\n刚才发生了什么？¶\nScrapy为Spider的 start_urls 属性中的每个URL创建了 scrapy.Request 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。\nRequest对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。\n\n\n\n提取Item¶\n\nSelectors选择器简介¶\n从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制:\nScrapy Selectors 。\n关于selector和其他提取机制的信息请参考 Selector文档 。\n这里给出XPath表达式的例子及对应的含义:\n\n/html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素\n/html/head/title/text(): 选择上面提到的 <title> 元素的文字\n//td: 选择所有的 <td> 元素\n//div[@class=\"mine\"]: 选择所有具有 class=\"mine\" 属性的 div 元素\n\n上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。\n如果您想了解的更多，我们推荐 这篇XPath教程 。\n为了配合XPath，Scrapy除了提供了 Selector\n之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。\nSelector有四个基本的方法(点击相应的方法可以看到详细的API文档):\n\nxpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。\ncss(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.\nextract(): 序列化该节点为unicode字符串并返回list。\nre(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。\n\n\n\n在Shell中尝试Selector选择器¶\n为了介绍Selector的使用方法，接下来我们将要使用内置的 Scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。\n您需要进入项目的根目录，执行下列命令来启动shell:\nscrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\n\n\n\n注解\n当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 & 字符)会导致Scrapy运行失败。\n\nshell的输出类似:\n[ ... Scrapy log here ... ]\n\n2015-01-07 22:01:53+0800 [domz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x02CE2530>\n[s]   item       {}\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   sel        <Selector xpath=None data=u'<html lang=\"en\">\\r\\n<head>\\r\\n<meta http-equ'>\n[s]   settings   <CrawlerSettings module=<module 'tutorial.settings' from 'tutorial\\settings.pyc'>>\n[s]   spider     <DomzSpider 'domz' at 0x302e350>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n>>>\n\n\n当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。\n更为重要的是，当输入 response.selector 时，\n您将获取到一个可以用于查询返回数据的selector(选择器)，\n以及映射到 response.selector.xpath() 、 response.selector.css() 的\n快捷方法(shortcut): response.xpath() 和 response.css() 。\n同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。\n让我们来试试:\nIn [1]: sel.xpath('//title')\nOut[1]: [<Selector xpath='//title' data=u'<title>Open Directory - Computers: Progr'>]\n\nIn [2]: sel.xpath('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: sel.xpath('//title/text()')\nOut[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]\n\nIn [4]: sel.xpath('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: sel.xpath('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\n\n提取数据¶\n现在，我们来尝试从这些页面中提取些有用的数据。\n您可以在终端中输入 response.body 来观察HTML源码并确定合适的XPath表达式。不过，这任务非常无聊且不易。您可以考虑使用Firefox的Firebug扩展来使得工作更为轻松。详情请参考 使用Firebug进行爬取 和 借助Firefox来爬取 。\n在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。\n我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:\nsel.xpath('//ul/li')\n\n\n网站的描述:\nsel.xpath('//ul/li/text()').extract()\n\n\n网站的标题:\nsel.xpath('//ul/li/a/text()').extract()\n\n\n以及网站的链接:\nsel.xpath('//ul/li/a/@href').extract()\n\n\n之前提到过，每个 .xpath() 调用返回selector组成的list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性:\nfor sel in response.xpath('//ul/li'):\n    title = sel.xpath('a/text()').extract()\n    link = sel.xpath('a/@href').extract()\n    desc = sel.xpath('text()').extract()\n    print title, link, desc\n\n\n\n注解\n关于嵌套selctor的更多详细信息，请参考 嵌套选择器(selectors) 以及 选择器(Selectors) 文档中的 使用相对XPaths 部分。\n\n在我们的spider中加入这段代码:\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            title = sel.xpath('a/text()').extract()\n            link = sel.xpath('a/@href').extract()\n            desc = sel.xpath('text()').extract()\n            print title, link, desc\n\n\n现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出:\nscrapy crawl dmoz\n\n\n\n\n\n使用item¶\nItem 对象是自定义的python字典。\n您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\n一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:\nimport scrapy\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\n\n\n\n注解\n您可以在 dirbot 项目中找到一个具有完整功能的spider。该项目可以通过 https://github.com/scrapy/dirbot 找到。\n\n现在对dmoz.org进行爬取将会产生 DmozItem 对象:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\n\n保存爬取到的数据¶\n最简单存储爬取的数据的方式是使用 Feed exports:\nscrapy crawl dmoz -o items.json\n\n\n该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。\n在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。\n如果需要对爬取到的item做更多更为复杂的操作，您可以编写\nItem Pipeline 。\n类似于我们在创建项目时对Item做的，用于您编写自己的\ntutorial/pipelines.py 也被创建。\n不过如果您仅仅想要保存item，您不需要实现任何的pipeline。\n\n\n下一步¶\n本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及。请查看 初窥Scrapy 章节中的 还有什么？ 部分,大致浏览大部分重要的特性。\n接着，我们推荐您把玩一个例子(查看 例子)，而后继续阅读 基本概念 。\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "使用item", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#id8", "content": ["\n            \n    \n  \nScrapy入门教程¶\n在本篇教程中，我们假定您已经安装好Scrapy。\n如若不然，请参考 安装指南 。\n接下来以 Open Directory Project(dmoz) (dmoz)\n为例来讲述爬取。\n本篇教程中将带您完成下列任务:\n\n创建一个Scrapy项目\n定义提取的Item\n编写爬取网站的 spider 并提取 Item\n编写 Item Pipeline 来存储提取到的Item(即数据)\n\nScrapy由 Python 编写。如果您刚接触并且好奇这门语言的特性以及Scrapy的详情，\n对于已经熟悉其他语言并且想快速学习Python的编程老手，\n我们推荐 Learn Python The Hard Way ，\n对于想从Python开始学习的编程新手，\n非程序员的Python学习资料列表 将是您的选择。\n\n创建项目¶\n在开始爬取之前，您必须创建一个新的Scrapy项目。\n进入您打算存储代码的目录中，运行下列命令:\nscrapy startproject tutorial\n\n\n该命令将会创建包含下列内容的 tutorial 目录:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\n\n这些文件分别是:\n\nscrapy.cfg: 项目的配置文件\ntutorial/: 该项目的python模块。之后您将在此加入代码。\ntutorial/items.py: 项目中的item文件.\ntutorial/pipelines.py: 项目中的pipelines文件.\ntutorial/settings.py: 项目的设置文件.\ntutorial/spiders/: 放置spider代码的目录.\n\n\n\n定义Item¶\nItem 是保存爬取到的数据的容器；其使用方法和python字典类似，\n并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类，\n并且定义类型为 scrapy.Field 的类属性来定义一个Item。\n(如果不了解ORM, 不用担心，您会发现这个步骤非常简单)\n首先根据需要从dmoz.org获取到的数据对item进行建模。\n我们需要从dmoz中获取名字，url，以及网站的描述。\n对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件:\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\n\n一开始这看起来可能有点复杂，但是通过定义item，\n您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。\n\n\n编写第一个爬虫(Spider)¶\nSpider是用户编写用于从单个网站(或者一些网站)爬取数据的类。\n其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容，\n提取生成 item 的方法。\n为了创建一个Spider，您必须继承 scrapy.Spider 类，\n且定义以下三个属性:\n\nname: 用于区别Spider。\n该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。\nstart_urls: 包含了Spider在启动时进行爬取的url列表。\n因此，第一个被获取到的页面将是其中之一。\n后续的URL则从初始的URL获取到的数据中提取。\nparse() 是spider的一个方法。\n被调用时，每个初始URL完成下载后生成的 Response\n对象将会作为唯一的参数传递给该函数。\n该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。\n\n以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中:\nimport scrapy\n\nclass DmozSpider(scrapy.spiders.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n\n\n\n爬取¶\n进入项目的根目录，执行下列命令启动spider:\nscrapy crawl dmoz\n\n\ncrawl dmoz 启动用于爬取 dmoz.org 的spider，您将得到类似的输出:\n2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)\n2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...\n2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened\n2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)\n\n\n查看包含 [dmoz] 的输出，可以看到输出的log中包含定义在 start_urls 的初始URL，并且与spider中是一一对应的。在log中可以看到其没有指向其他页面( (referer:None) )。\n除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含url所对应的内容的文件被创建了: Book , Resources 。\n\n刚才发生了什么？¶\nScrapy为Spider的 start_urls 属性中的每个URL创建了 scrapy.Request 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。\nRequest对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。\n\n\n\n提取Item¶\n\nSelectors选择器简介¶\n从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制:\nScrapy Selectors 。\n关于selector和其他提取机制的信息请参考 Selector文档 。\n这里给出XPath表达式的例子及对应的含义:\n\n/html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素\n/html/head/title/text(): 选择上面提到的 <title> 元素的文字\n//td: 选择所有的 <td> 元素\n//div[@class=\"mine\"]: 选择所有具有 class=\"mine\" 属性的 div 元素\n\n上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。\n如果您想了解的更多，我们推荐 这篇XPath教程 。\n为了配合XPath，Scrapy除了提供了 Selector\n之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。\nSelector有四个基本的方法(点击相应的方法可以看到详细的API文档):\n\nxpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。\ncss(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.\nextract(): 序列化该节点为unicode字符串并返回list。\nre(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。\n\n\n\n在Shell中尝试Selector选择器¶\n为了介绍Selector的使用方法，接下来我们将要使用内置的 Scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。\n您需要进入项目的根目录，执行下列命令来启动shell:\nscrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\n\n\n\n注解\n当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 & 字符)会导致Scrapy运行失败。\n\nshell的输出类似:\n[ ... Scrapy log here ... ]\n\n2015-01-07 22:01:53+0800 [domz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x02CE2530>\n[s]   item       {}\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   sel        <Selector xpath=None data=u'<html lang=\"en\">\\r\\n<head>\\r\\n<meta http-equ'>\n[s]   settings   <CrawlerSettings module=<module 'tutorial.settings' from 'tutorial\\settings.pyc'>>\n[s]   spider     <DomzSpider 'domz' at 0x302e350>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n>>>\n\n\n当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。\n更为重要的是，当输入 response.selector 时，\n您将获取到一个可以用于查询返回数据的selector(选择器)，\n以及映射到 response.selector.xpath() 、 response.selector.css() 的\n快捷方法(shortcut): response.xpath() 和 response.css() 。\n同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。\n让我们来试试:\nIn [1]: sel.xpath('//title')\nOut[1]: [<Selector xpath='//title' data=u'<title>Open Directory - Computers: Progr'>]\n\nIn [2]: sel.xpath('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: sel.xpath('//title/text()')\nOut[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]\n\nIn [4]: sel.xpath('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: sel.xpath('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\n\n提取数据¶\n现在，我们来尝试从这些页面中提取些有用的数据。\n您可以在终端中输入 response.body 来观察HTML源码并确定合适的XPath表达式。不过，这任务非常无聊且不易。您可以考虑使用Firefox的Firebug扩展来使得工作更为轻松。详情请参考 使用Firebug进行爬取 和 借助Firefox来爬取 。\n在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。\n我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:\nsel.xpath('//ul/li')\n\n\n网站的描述:\nsel.xpath('//ul/li/text()').extract()\n\n\n网站的标题:\nsel.xpath('//ul/li/a/text()').extract()\n\n\n以及网站的链接:\nsel.xpath('//ul/li/a/@href').extract()\n\n\n之前提到过，每个 .xpath() 调用返回selector组成的list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性:\nfor sel in response.xpath('//ul/li'):\n    title = sel.xpath('a/text()').extract()\n    link = sel.xpath('a/@href').extract()\n    desc = sel.xpath('text()').extract()\n    print title, link, desc\n\n\n\n注解\n关于嵌套selctor的更多详细信息，请参考 嵌套选择器(selectors) 以及 选择器(Selectors) 文档中的 使用相对XPaths 部分。\n\n在我们的spider中加入这段代码:\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            title = sel.xpath('a/text()').extract()\n            link = sel.xpath('a/@href').extract()\n            desc = sel.xpath('text()').extract()\n            print title, link, desc\n\n\n现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出:\nscrapy crawl dmoz\n\n\n\n\n\n使用item¶\nItem 对象是自定义的python字典。\n您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\n一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:\nimport scrapy\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\n\n\n\n注解\n您可以在 dirbot 项目中找到一个具有完整功能的spider。该项目可以通过 https://github.com/scrapy/dirbot 找到。\n\n现在对dmoz.org进行爬取将会产生 DmozItem 对象:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\n\n保存爬取到的数据¶\n最简单存储爬取的数据的方式是使用 Feed exports:\nscrapy crawl dmoz -o items.json\n\n\n该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。\n在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。\n如果需要对爬取到的item做更多更为复杂的操作，您可以编写\nItem Pipeline 。\n类似于我们在创建项目时对Item做的，用于您编写自己的\ntutorial/pipelines.py 也被创建。\n不过如果您仅仅想要保存item，您不需要实现任何的pipeline。\n\n\n下一步¶\n本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及。请查看 初窥Scrapy 章节中的 还有什么？ 部分,大致浏览大部分重要的特性。\n接着，我们推荐您把玩一个例子(查看 例子)，而后继续阅读 基本概念 。\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "保存爬取到的数据", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#id9", "content": ["\n            \n    \n  \nScrapy入门教程¶\n在本篇教程中，我们假定您已经安装好Scrapy。\n如若不然，请参考 安装指南 。\n接下来以 Open Directory Project(dmoz) (dmoz)\n为例来讲述爬取。\n本篇教程中将带您完成下列任务:\n\n创建一个Scrapy项目\n定义提取的Item\n编写爬取网站的 spider 并提取 Item\n编写 Item Pipeline 来存储提取到的Item(即数据)\n\nScrapy由 Python 编写。如果您刚接触并且好奇这门语言的特性以及Scrapy的详情，\n对于已经熟悉其他语言并且想快速学习Python的编程老手，\n我们推荐 Learn Python The Hard Way ，\n对于想从Python开始学习的编程新手，\n非程序员的Python学习资料列表 将是您的选择。\n\n创建项目¶\n在开始爬取之前，您必须创建一个新的Scrapy项目。\n进入您打算存储代码的目录中，运行下列命令:\nscrapy startproject tutorial\n\n\n该命令将会创建包含下列内容的 tutorial 目录:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\n\n这些文件分别是:\n\nscrapy.cfg: 项目的配置文件\ntutorial/: 该项目的python模块。之后您将在此加入代码。\ntutorial/items.py: 项目中的item文件.\ntutorial/pipelines.py: 项目中的pipelines文件.\ntutorial/settings.py: 项目的设置文件.\ntutorial/spiders/: 放置spider代码的目录.\n\n\n\n定义Item¶\nItem 是保存爬取到的数据的容器；其使用方法和python字典类似，\n并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类，\n并且定义类型为 scrapy.Field 的类属性来定义一个Item。\n(如果不了解ORM, 不用担心，您会发现这个步骤非常简单)\n首先根据需要从dmoz.org获取到的数据对item进行建模。\n我们需要从dmoz中获取名字，url，以及网站的描述。\n对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件:\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\n\n一开始这看起来可能有点复杂，但是通过定义item，\n您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。\n\n\n编写第一个爬虫(Spider)¶\nSpider是用户编写用于从单个网站(或者一些网站)爬取数据的类。\n其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容，\n提取生成 item 的方法。\n为了创建一个Spider，您必须继承 scrapy.Spider 类，\n且定义以下三个属性:\n\nname: 用于区别Spider。\n该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。\nstart_urls: 包含了Spider在启动时进行爬取的url列表。\n因此，第一个被获取到的页面将是其中之一。\n后续的URL则从初始的URL获取到的数据中提取。\nparse() 是spider的一个方法。\n被调用时，每个初始URL完成下载后生成的 Response\n对象将会作为唯一的参数传递给该函数。\n该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。\n\n以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中:\nimport scrapy\n\nclass DmozSpider(scrapy.spiders.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n\n\n\n爬取¶\n进入项目的根目录，执行下列命令启动spider:\nscrapy crawl dmoz\n\n\ncrawl dmoz 启动用于爬取 dmoz.org 的spider，您将得到类似的输出:\n2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)\n2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...\n2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened\n2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)\n\n\n查看包含 [dmoz] 的输出，可以看到输出的log中包含定义在 start_urls 的初始URL，并且与spider中是一一对应的。在log中可以看到其没有指向其他页面( (referer:None) )。\n除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含url所对应的内容的文件被创建了: Book , Resources 。\n\n刚才发生了什么？¶\nScrapy为Spider的 start_urls 属性中的每个URL创建了 scrapy.Request 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。\nRequest对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。\n\n\n\n提取Item¶\n\nSelectors选择器简介¶\n从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制:\nScrapy Selectors 。\n关于selector和其他提取机制的信息请参考 Selector文档 。\n这里给出XPath表达式的例子及对应的含义:\n\n/html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素\n/html/head/title/text(): 选择上面提到的 <title> 元素的文字\n//td: 选择所有的 <td> 元素\n//div[@class=\"mine\"]: 选择所有具有 class=\"mine\" 属性的 div 元素\n\n上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。\n如果您想了解的更多，我们推荐 这篇XPath教程 。\n为了配合XPath，Scrapy除了提供了 Selector\n之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。\nSelector有四个基本的方法(点击相应的方法可以看到详细的API文档):\n\nxpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。\ncss(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.\nextract(): 序列化该节点为unicode字符串并返回list。\nre(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。\n\n\n\n在Shell中尝试Selector选择器¶\n为了介绍Selector的使用方法，接下来我们将要使用内置的 Scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。\n您需要进入项目的根目录，执行下列命令来启动shell:\nscrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\n\n\n\n注解\n当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 & 字符)会导致Scrapy运行失败。\n\nshell的输出类似:\n[ ... Scrapy log here ... ]\n\n2015-01-07 22:01:53+0800 [domz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x02CE2530>\n[s]   item       {}\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   sel        <Selector xpath=None data=u'<html lang=\"en\">\\r\\n<head>\\r\\n<meta http-equ'>\n[s]   settings   <CrawlerSettings module=<module 'tutorial.settings' from 'tutorial\\settings.pyc'>>\n[s]   spider     <DomzSpider 'domz' at 0x302e350>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n>>>\n\n\n当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。\n更为重要的是，当输入 response.selector 时，\n您将获取到一个可以用于查询返回数据的selector(选择器)，\n以及映射到 response.selector.xpath() 、 response.selector.css() 的\n快捷方法(shortcut): response.xpath() 和 response.css() 。\n同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。\n让我们来试试:\nIn [1]: sel.xpath('//title')\nOut[1]: [<Selector xpath='//title' data=u'<title>Open Directory - Computers: Progr'>]\n\nIn [2]: sel.xpath('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: sel.xpath('//title/text()')\nOut[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]\n\nIn [4]: sel.xpath('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: sel.xpath('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\n\n提取数据¶\n现在，我们来尝试从这些页面中提取些有用的数据。\n您可以在终端中输入 response.body 来观察HTML源码并确定合适的XPath表达式。不过，这任务非常无聊且不易。您可以考虑使用Firefox的Firebug扩展来使得工作更为轻松。详情请参考 使用Firebug进行爬取 和 借助Firefox来爬取 。\n在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。\n我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:\nsel.xpath('//ul/li')\n\n\n网站的描述:\nsel.xpath('//ul/li/text()').extract()\n\n\n网站的标题:\nsel.xpath('//ul/li/a/text()').extract()\n\n\n以及网站的链接:\nsel.xpath('//ul/li/a/@href').extract()\n\n\n之前提到过，每个 .xpath() 调用返回selector组成的list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性:\nfor sel in response.xpath('//ul/li'):\n    title = sel.xpath('a/text()').extract()\n    link = sel.xpath('a/@href').extract()\n    desc = sel.xpath('text()').extract()\n    print title, link, desc\n\n\n\n注解\n关于嵌套selctor的更多详细信息，请参考 嵌套选择器(selectors) 以及 选择器(Selectors) 文档中的 使用相对XPaths 部分。\n\n在我们的spider中加入这段代码:\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            title = sel.xpath('a/text()').extract()\n            link = sel.xpath('a/@href').extract()\n            desc = sel.xpath('text()').extract()\n            print title, link, desc\n\n\n现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出:\nscrapy crawl dmoz\n\n\n\n\n\n使用item¶\nItem 对象是自定义的python字典。\n您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\n一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:\nimport scrapy\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\n\n\n\n注解\n您可以在 dirbot 项目中找到一个具有完整功能的spider。该项目可以通过 https://github.com/scrapy/dirbot 找到。\n\n现在对dmoz.org进行爬取将会产生 DmozItem 对象:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\n\n保存爬取到的数据¶\n最简单存储爬取的数据的方式是使用 Feed exports:\nscrapy crawl dmoz -o items.json\n\n\n该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。\n在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。\n如果需要对爬取到的item做更多更为复杂的操作，您可以编写\nItem Pipeline 。\n类似于我们在创建项目时对Item做的，用于您编写自己的\ntutorial/pipelines.py 也被创建。\n不过如果您仅仅想要保存item，您不需要实现任何的pipeline。\n\n\n下一步¶\n本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及。请查看 初窥Scrapy 章节中的 还有什么？ 部分,大致浏览大部分重要的特性。\n接着，我们推荐您把玩一个例子(查看 例子)，而后继续阅读 基本概念 。\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "下一步", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html#id10", "content": ["\n            \n    \n  \nScrapy入门教程¶\n在本篇教程中，我们假定您已经安装好Scrapy。\n如若不然，请参考 安装指南 。\n接下来以 Open Directory Project(dmoz) (dmoz)\n为例来讲述爬取。\n本篇教程中将带您完成下列任务:\n\n创建一个Scrapy项目\n定义提取的Item\n编写爬取网站的 spider 并提取 Item\n编写 Item Pipeline 来存储提取到的Item(即数据)\n\nScrapy由 Python 编写。如果您刚接触并且好奇这门语言的特性以及Scrapy的详情，\n对于已经熟悉其他语言并且想快速学习Python的编程老手，\n我们推荐 Learn Python The Hard Way ，\n对于想从Python开始学习的编程新手，\n非程序员的Python学习资料列表 将是您的选择。\n\n创建项目¶\n在开始爬取之前，您必须创建一个新的Scrapy项目。\n进入您打算存储代码的目录中，运行下列命令:\nscrapy startproject tutorial\n\n\n该命令将会创建包含下列内容的 tutorial 目录:\ntutorial/\n    scrapy.cfg\n    tutorial/\n        __init__.py\n        items.py\n        pipelines.py\n        settings.py\n        spiders/\n            __init__.py\n            ...\n\n\n这些文件分别是:\n\nscrapy.cfg: 项目的配置文件\ntutorial/: 该项目的python模块。之后您将在此加入代码。\ntutorial/items.py: 项目中的item文件.\ntutorial/pipelines.py: 项目中的pipelines文件.\ntutorial/settings.py: 项目的设置文件.\ntutorial/spiders/: 放置spider代码的目录.\n\n\n\n定义Item¶\nItem 是保存爬取到的数据的容器；其使用方法和python字典类似，\n并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。\n类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类，\n并且定义类型为 scrapy.Field 的类属性来定义一个Item。\n(如果不了解ORM, 不用担心，您会发现这个步骤非常简单)\n首先根据需要从dmoz.org获取到的数据对item进行建模。\n我们需要从dmoz中获取名字，url，以及网站的描述。\n对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件:\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\n\n一开始这看起来可能有点复杂，但是通过定义item，\n您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义。\n\n\n编写第一个爬虫(Spider)¶\nSpider是用户编写用于从单个网站(或者一些网站)爬取数据的类。\n其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容，\n提取生成 item 的方法。\n为了创建一个Spider，您必须继承 scrapy.Spider 类，\n且定义以下三个属性:\n\nname: 用于区别Spider。\n该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。\nstart_urls: 包含了Spider在启动时进行爬取的url列表。\n因此，第一个被获取到的页面将是其中之一。\n后续的URL则从初始的URL获取到的数据中提取。\nparse() 是spider的一个方法。\n被调用时，每个初始URL完成下载后生成的 Response\n对象将会作为唯一的参数传递给该函数。\n该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。\n\n以下为我们的第一个Spider代码，保存在 tutorial/spiders 目录下的 dmoz_spider.py 文件中:\nimport scrapy\n\nclass DmozSpider(scrapy.spiders.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        filename = response.url.split(\"/\")[-2]\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n\n\n\n爬取¶\n进入项目的根目录，执行下列命令启动spider:\nscrapy crawl dmoz\n\n\ncrawl dmoz 启动用于爬取 dmoz.org 的spider，您将得到类似的输出:\n2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)\n2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...\n2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...\n2014-01-23 18:13:07-0400 [dmoz] INFO: Spider opened\n2014-01-23 18:13:08-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2014-01-23 18:13:09-0400 [dmoz] INFO: Closing spider (finished)\n\n\n查看包含 [dmoz] 的输出，可以看到输出的log中包含定义在 start_urls 的初始URL，并且与spider中是一一对应的。在log中可以看到其没有指向其他页面( (referer:None) )。\n除此之外，更有趣的事情发生了。就像我们 parse 方法指定的那样，有两个包含url所对应的内容的文件被创建了: Book , Resources 。\n\n刚才发生了什么？¶\nScrapy为Spider的 start_urls 属性中的每个URL创建了 scrapy.Request 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。\nRequest对象经过调度，执行生成 scrapy.http.Response 对象并送回给spider parse() 方法。\n\n\n\n提取Item¶\n\nSelectors选择器简介¶\n从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制:\nScrapy Selectors 。\n关于selector和其他提取机制的信息请参考 Selector文档 。\n这里给出XPath表达式的例子及对应的含义:\n\n/html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素\n/html/head/title/text(): 选择上面提到的 <title> 元素的文字\n//td: 选择所有的 <td> 元素\n//div[@class=\"mine\"]: 选择所有具有 class=\"mine\" 属性的 div 元素\n\n上边仅仅是几个简单的XPath例子，XPath实际上要比这远远强大的多。\n如果您想了解的更多，我们推荐 这篇XPath教程 。\n为了配合XPath，Scrapy除了提供了 Selector\n之外，还提供了方法来避免每次从response中提取数据时生成selector的麻烦。\nSelector有四个基本的方法(点击相应的方法可以看到详细的API文档):\n\nxpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。\ncss(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.\nextract(): 序列化该节点为unicode字符串并返回list。\nre(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。\n\n\n\n在Shell中尝试Selector选择器¶\n为了介绍Selector的使用方法，接下来我们将要使用内置的 Scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。\n您需要进入项目的根目录，执行下列命令来启动shell:\nscrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\n\n\n\n注解\n当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 & 字符)会导致Scrapy运行失败。\n\nshell的输出类似:\n[ ... Scrapy log here ... ]\n\n2015-01-07 22:01:53+0800 [domz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x02CE2530>\n[s]   item       {}\n[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n[s]   sel        <Selector xpath=None data=u'<html lang=\"en\">\\r\\n<head>\\r\\n<meta http-equ'>\n[s]   settings   <CrawlerSettings module=<module 'tutorial.settings' from 'tutorial\\settings.pyc'>>\n[s]   spider     <DomzSpider 'domz' at 0x302e350>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\n>>>\n\n\n当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。\n更为重要的是，当输入 response.selector 时，\n您将获取到一个可以用于查询返回数据的selector(选择器)，\n以及映射到 response.selector.xpath() 、 response.selector.css() 的\n快捷方法(shortcut): response.xpath() 和 response.css() 。\n同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。\n让我们来试试:\nIn [1]: sel.xpath('//title')\nOut[1]: [<Selector xpath='//title' data=u'<title>Open Directory - Computers: Progr'>]\n\nIn [2]: sel.xpath('//title').extract()\nOut[2]: [u'<title>Open Directory - Computers: Programming: Languages: Python: Books</title>']\n\nIn [3]: sel.xpath('//title/text()')\nOut[3]: [<Selector xpath='//title/text()' data=u'Open Directory - Computers: Programming:'>]\n\nIn [4]: sel.xpath('//title/text()').extract()\nOut[4]: [u'Open Directory - Computers: Programming: Languages: Python: Books']\n\nIn [5]: sel.xpath('//title/text()').re('(\\w+):')\nOut[5]: [u'Computers', u'Programming', u'Languages', u'Python']\n\n\n\n\n提取数据¶\n现在，我们来尝试从这些页面中提取些有用的数据。\n您可以在终端中输入 response.body 来观察HTML源码并确定合适的XPath表达式。不过，这任务非常无聊且不易。您可以考虑使用Firefox的Firebug扩展来使得工作更为轻松。详情请参考 使用Firebug进行爬取 和 借助Firefox来爬取 。\n在查看了网页的源码后，您会发现网站的信息是被包含在 第二个 <ul> 元素中。\n我们可以通过这段代码选择该页面中网站列表里所有 <li> 元素:\nsel.xpath('//ul/li')\n\n\n网站的描述:\nsel.xpath('//ul/li/text()').extract()\n\n\n网站的标题:\nsel.xpath('//ul/li/a/text()').extract()\n\n\n以及网站的链接:\nsel.xpath('//ul/li/a/@href').extract()\n\n\n之前提到过，每个 .xpath() 调用返回selector组成的list，因此我们可以拼接更多的 .xpath() 来进一步获取某个节点。我们将在下边使用这样的特性:\nfor sel in response.xpath('//ul/li'):\n    title = sel.xpath('a/text()').extract()\n    link = sel.xpath('a/@href').extract()\n    desc = sel.xpath('text()').extract()\n    print title, link, desc\n\n\n\n注解\n关于嵌套selctor的更多详细信息，请参考 嵌套选择器(selectors) 以及 选择器(Selectors) 文档中的 使用相对XPaths 部分。\n\n在我们的spider中加入这段代码:\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            title = sel.xpath('a/text()').extract()\n            link = sel.xpath('a/@href').extract()\n            desc = sel.xpath('text()').extract()\n            print title, link, desc\n\n\n现在尝试再次爬取dmoz.org，您将看到爬取到的网站信息被成功输出:\nscrapy crawl dmoz\n\n\n\n\n\n使用item¶\nItem 对象是自定义的python字典。\n您可以使用标准的字典语法来获取到其每个字段的值。(字段即是我们之前用Field赋值的属性):\n>>> item = DmozItem()\n>>> item['title'] = 'Example title'\n>>> item['title']\n'Example title'\n\n\n一般来说，Spider将会将爬取到的数据以 Item 对象返回。所以为了将爬取的数据返回，我们最终的代码将是:\nimport scrapy\n\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\n\n\n\n注解\n您可以在 dirbot 项目中找到一个具有完整功能的spider。该项目可以通过 https://github.com/scrapy/dirbot 找到。\n\n现在对dmoz.org进行爬取将会产生 DmozItem 对象:\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\\n],\n      'link': [u'http://gnosis.cx/TPiP/'],\n      'title': [u'Text Processing in Python']}\n[dmoz] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>\n     {'desc': [u' - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\\n'],\n      'link': [u'http://www.informit.com/store/product.aspx?isbn=0130211192'],\n      'title': [u'XML Processing with Python']}\n\n\n\n\n\n保存爬取到的数据¶\n最简单存储爬取的数据的方式是使用 Feed exports:\nscrapy crawl dmoz -o items.json\n\n\n该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。\n在类似本篇教程里这样小规模的项目中，这种存储方式已经足够。\n如果需要对爬取到的item做更多更为复杂的操作，您可以编写\nItem Pipeline 。\n类似于我们在创建项目时对Item做的，用于您编写自己的\ntutorial/pipelines.py 也被创建。\n不过如果您仅仅想要保存item，您不需要实现任何的pipeline。\n\n\n下一步¶\n本篇教程仅介绍了Scrapy的基础，还有很多特性没有涉及。请查看 初窥Scrapy 章节中的 还有什么？ 部分,大致浏览大部分重要的特性。\n接着，我们推荐您把玩一个例子(查看 例子)，而后继续阅读 基本概念 。\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "发送email", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/email.html", "content": ["\n            \n    \n  \n例子¶\n学习的最好方法就是参考例子，Scrapy也不例外。Scrapy提供了一个叫做 dirbot 的样例项目供您把玩学习。其包含了在教程中介绍的dmoz spider。\n您可以通过 https://github.com/scrapy/dirbot 找到 dirbot 。其包含了README文件，详细介绍了项目的内容。\n如果您熟悉git，您可以checkout代码。或者您可以点击 Downloads 来下载项目的tarball或者zip的压缩包。\nSnipplr上的scrapy标签 是用来分享spider，middeware，extension或者script代码片段。欢迎(并鼓励)在那分享您的代码。\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Telnet终端(Telnet Console)", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "如何访问telnet终端", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html#telnet", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "telnet终端中可用的变量", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html#id1", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Telnet console usage examples", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html#telnet-console-usage-examples", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "查看引擎状态", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html#id2", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "暂停，恢复和停止Scrapy引擎", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html#scrapy", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Telnet终端信号", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html#id3", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Telnet设定", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html#id4", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "TELNETCONSOLE_PORT", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html#telnetconsole-port", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "TELNETCONSOLE_HOST", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/telnetconsole.html#telnetconsole-host", "content": ["\n            \n    \n  \nTelnet终端(Telnet Console)¶\nScrapy提供了内置的telnet终端，以供检查，控制Scrapy运行的进程。\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以在其中做任何事。\ntelnet终端是一个\n自带的Scrapy扩展 。\n该扩展默认为启用，不过您也可以关闭。\n关于扩展的更多内容请参考\nTelnet console 扩展 。\n\n如何访问telnet终端¶\ntelnet终端监听设置中定义的\nTELNETCONSOLE_PORT ，默认为 6023 。\n访问telnet请输入:\ntelnet localhost 6023\n>>>\n\n\nWindows及大多数Linux发行版都自带了所需的telnet程序。\n\n\ntelnet终端中可用的变量¶\ntelnet仅仅是一个运行在Scrapy进程中的普通python终端。因此您可以做任何事情，甚至是导入新终端。\ntelnet为了方便提供了一些默认定义的变量:\n\n\n\n\n\n\n快捷名称\n描述\n\n\n\ncrawler\nScrapy Crawler (scrapy.crawler.Crawler 对象)\n\nengine\nCrawler.engine属性\n\nspider\n当前激活的爬虫(spider)\n\nslot\nthe engine slot\n\nextensions\n扩展管理器(manager) (Crawler.extensions属性)\n\nstats\n状态收集器 (Crawler.stats属性)\n\nsettings\nScrapy设置(setting)对象 (Crawler.settings属性)\n\nest\n打印引擎状态的报告\n\nprefs\n针对内存调试 (参考 调试内存溢出)\n\np\npprint.pprint 函数的简写\n\nhpy\n针对内存调试 (参考 调试内存溢出)\n\n\n\n\n\nTelnet console usage examples¶\n下面是使用telnet终端的一些例子:\n\n查看引擎状态¶\n在终端中您可以使用Scrapy引擎的 est() 方法来快速查看状态:\ntelnet localhost 6023\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 8.62972998619\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 16\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : followall\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : False\nlen(engine.slot.inprogress)                     : 16\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 92\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 0\nengine.scraper.slot.active_size                 : 0\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\n\n\n\n\n暂停，恢复和停止Scrapy引擎¶\n暂停:\ntelnet localhost 6023\n>>> engine.pause()\n>>>\n\n\n恢复:\ntelnet localhost 6023\n>>> engine.unpause()\n>>>\n\n\n停止:\ntelnet localhost 6023\n>>> engine.stop()\nConnection closed by foreign host.\n\n\n\n\n\nTelnet终端信号¶\n\n\nscrapy.telnet.update_telnet_vars(telnet_vars)¶\n在telnet终端开启前发送该信号。您可以挂载(hook up)该信号来添加，移除或更新\ntelnet本地命名空间可用的变量。\n您可以通过在您的处理函数(handler)中更新 telnet_vars 字典来实现该修改。\n\n\n\n\n参数:telnet_vars (dict) – telnet变量的字典\n\n\n\n\n\n\n\nTelnet设定¶\n以下是终端的一些设定:\n\nTELNETCONSOLE_PORT¶\nDefault: [6023, 6073]\ntelnet终端使用的端口范围。如果设为 None 或 0 ，\n则动态分配端口。\n\n\nTELNETCONSOLE_HOST¶\n默认: '127.0.0.1'\ntelnet终端监听的接口(interface)。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "下载项目图片", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/images.html", "content": ["\n            \n    \n  \nWeb Service¶\nwebserver 被移动到另外一个项目中。\n托管在:\n\nhttps://github.com/scrapy/scrapy-jsonrpc\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Ubuntu 软件包", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/ubuntu.html", "content": ["\n            \n    \n  \nUbuntu 软件包¶\n\n0.10 新版功能.\n\nScrapinghub 发布的apt-get可获取版本通常比Ubuntu里更新，并且在比 Github 仓库\n(master & stable branches) 稳定的同时还包括了最新的漏洞修复。\n用法:\n\n把Scrapy签名的GPG密钥添加到APT的钥匙环中:\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 627220E7\n\n\n\n执行如下命令，创建 /etc/apt/sources.list.d/scrapy.list 文件:\necho 'deb http://archive.scrapy.org/ubuntu scrapy main' | sudo tee /etc/apt/sources.list.d/scrapy.list\n\n\n\n更新包列表并安装 scrapy-0.25:\n\nsudo apt-get update && sudo apt-get install scrapy-0.25\n\n\n\n\n注解\n如果你要升级Scrapy，请重复步骤3。\n\n\n警告\ndebian官方源提供的 python-scrapy 是一个非常老的版本且不再获得Scrapy团队支持。\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Scrapyd", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/scrapyd.html", "content": ["\n            \n    \n  \nScrapyd¶\nScrapyd被移动成为一个单独的项目。\n其文档当前被托管在:\n\nhttp://scrapyd.readthedocs.org/\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "自动限速(AutoThrottle)扩展", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html", "content": ["\n            \n    \n  \n自动限速(AutoThrottle)扩展¶\n该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。\n\n设计目标¶\n\n更友好的对待网站，而不使用默认的下载延迟0。\n自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。\n用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。\n\n\n\n扩展是如何实现的¶\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。\n不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n限速算法¶\n算法根据以下规则调整下载延迟及并发数:\n\nspider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。\n当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。\n\n\n注解\nAutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比\nDOWNLOAD_DELAY 更低的下载延迟或者比\nCONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数\n(或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。\n\n\n\n设置¶\n下面是控制AutoThrottle扩展的设置:\n\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_MAX_DELAY\nAUTOTHROTTLE_DEBUG\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nDOWNLOAD_DELAY\n\n更多内容请参考 限速算法 。\n\nAUTOTHROTTLE_ENABLED¶\n默认: False\n启用AutoThrottle扩展。\n\n\nAUTOTHROTTLE_START_DELAY¶\n默认: 5.0\n初始下载延迟(单位:秒)。\n\n\nAUTOTHROTTLE_MAX_DELAY¶\n默认: 60.0\n在高延迟情况下最大的下载延迟(单位秒)。\n\n\nAUTOTHROTTLE_DEBUG¶\n默认: False\n起用AutoThrottle调试(debug)模式，展示每个接收到的response。\n您可以通过此来查看限速参数是如何实时被调整的。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "设计目标", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html#id1", "content": ["\n            \n    \n  \n自动限速(AutoThrottle)扩展¶\n该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。\n\n设计目标¶\n\n更友好的对待网站，而不使用默认的下载延迟0。\n自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。\n用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。\n\n\n\n扩展是如何实现的¶\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。\n不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n限速算法¶\n算法根据以下规则调整下载延迟及并发数:\n\nspider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。\n当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。\n\n\n注解\nAutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比\nDOWNLOAD_DELAY 更低的下载延迟或者比\nCONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数\n(或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。\n\n\n\n设置¶\n下面是控制AutoThrottle扩展的设置:\n\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_MAX_DELAY\nAUTOTHROTTLE_DEBUG\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nDOWNLOAD_DELAY\n\n更多内容请参考 限速算法 。\n\nAUTOTHROTTLE_ENABLED¶\n默认: False\n启用AutoThrottle扩展。\n\n\nAUTOTHROTTLE_START_DELAY¶\n默认: 5.0\n初始下载延迟(单位:秒)。\n\n\nAUTOTHROTTLE_MAX_DELAY¶\n默认: 60.0\n在高延迟情况下最大的下载延迟(单位秒)。\n\n\nAUTOTHROTTLE_DEBUG¶\n默认: False\n起用AutoThrottle调试(debug)模式，展示每个接收到的response。\n您可以通过此来查看限速参数是如何实时被调整的。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "扩展是如何实现的", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html#id2", "content": ["\n            \n    \n  \n自动限速(AutoThrottle)扩展¶\n该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。\n\n设计目标¶\n\n更友好的对待网站，而不使用默认的下载延迟0。\n自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。\n用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。\n\n\n\n扩展是如何实现的¶\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。\n不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n限速算法¶\n算法根据以下规则调整下载延迟及并发数:\n\nspider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。\n当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。\n\n\n注解\nAutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比\nDOWNLOAD_DELAY 更低的下载延迟或者比\nCONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数\n(或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。\n\n\n\n设置¶\n下面是控制AutoThrottle扩展的设置:\n\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_MAX_DELAY\nAUTOTHROTTLE_DEBUG\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nDOWNLOAD_DELAY\n\n更多内容请参考 限速算法 。\n\nAUTOTHROTTLE_ENABLED¶\n默认: False\n启用AutoThrottle扩展。\n\n\nAUTOTHROTTLE_START_DELAY¶\n默认: 5.0\n初始下载延迟(单位:秒)。\n\n\nAUTOTHROTTLE_MAX_DELAY¶\n默认: 60.0\n在高延迟情况下最大的下载延迟(单位秒)。\n\n\nAUTOTHROTTLE_DEBUG¶\n默认: False\n起用AutoThrottle调试(debug)模式，展示每个接收到的response。\n您可以通过此来查看限速参数是如何实时被调整的。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "限速算法", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html#autothrottle-algorithm", "content": ["\n            \n    \n  \n自动限速(AutoThrottle)扩展¶\n该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。\n\n设计目标¶\n\n更友好的对待网站，而不使用默认的下载延迟0。\n自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。\n用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。\n\n\n\n扩展是如何实现的¶\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。\n不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n限速算法¶\n算法根据以下规则调整下载延迟及并发数:\n\nspider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。\n当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。\n\n\n注解\nAutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比\nDOWNLOAD_DELAY 更低的下载延迟或者比\nCONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数\n(或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。\n\n\n\n设置¶\n下面是控制AutoThrottle扩展的设置:\n\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_MAX_DELAY\nAUTOTHROTTLE_DEBUG\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nDOWNLOAD_DELAY\n\n更多内容请参考 限速算法 。\n\nAUTOTHROTTLE_ENABLED¶\n默认: False\n启用AutoThrottle扩展。\n\n\nAUTOTHROTTLE_START_DELAY¶\n默认: 5.0\n初始下载延迟(单位:秒)。\n\n\nAUTOTHROTTLE_MAX_DELAY¶\n默认: 60.0\n在高延迟情况下最大的下载延迟(单位秒)。\n\n\nAUTOTHROTTLE_DEBUG¶\n默认: False\n起用AutoThrottle调试(debug)模式，展示每个接收到的response。\n您可以通过此来查看限速参数是如何实时被调整的。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "设置", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html#id4", "content": ["\n            \n    \n  \n自动限速(AutoThrottle)扩展¶\n该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。\n\n设计目标¶\n\n更友好的对待网站，而不使用默认的下载延迟0。\n自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。\n用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。\n\n\n\n扩展是如何实现的¶\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。\n不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n限速算法¶\n算法根据以下规则调整下载延迟及并发数:\n\nspider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。\n当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。\n\n\n注解\nAutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比\nDOWNLOAD_DELAY 更低的下载延迟或者比\nCONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数\n(或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。\n\n\n\n设置¶\n下面是控制AutoThrottle扩展的设置:\n\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_MAX_DELAY\nAUTOTHROTTLE_DEBUG\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nDOWNLOAD_DELAY\n\n更多内容请参考 限速算法 。\n\nAUTOTHROTTLE_ENABLED¶\n默认: False\n启用AutoThrottle扩展。\n\n\nAUTOTHROTTLE_START_DELAY¶\n默认: 5.0\n初始下载延迟(单位:秒)。\n\n\nAUTOTHROTTLE_MAX_DELAY¶\n默认: 60.0\n在高延迟情况下最大的下载延迟(单位秒)。\n\n\nAUTOTHROTTLE_DEBUG¶\n默认: False\n起用AutoThrottle调试(debug)模式，展示每个接收到的response。\n您可以通过此来查看限速参数是如何实时被调整的。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "AUTOTHROTTLE_ENABLED", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html#autothrottle-enabled", "content": ["\n            \n    \n  \n自动限速(AutoThrottle)扩展¶\n该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。\n\n设计目标¶\n\n更友好的对待网站，而不使用默认的下载延迟0。\n自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。\n用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。\n\n\n\n扩展是如何实现的¶\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。\n不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n限速算法¶\n算法根据以下规则调整下载延迟及并发数:\n\nspider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。\n当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。\n\n\n注解\nAutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比\nDOWNLOAD_DELAY 更低的下载延迟或者比\nCONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数\n(或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。\n\n\n\n设置¶\n下面是控制AutoThrottle扩展的设置:\n\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_MAX_DELAY\nAUTOTHROTTLE_DEBUG\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nDOWNLOAD_DELAY\n\n更多内容请参考 限速算法 。\n\nAUTOTHROTTLE_ENABLED¶\n默认: False\n启用AutoThrottle扩展。\n\n\nAUTOTHROTTLE_START_DELAY¶\n默认: 5.0\n初始下载延迟(单位:秒)。\n\n\nAUTOTHROTTLE_MAX_DELAY¶\n默认: 60.0\n在高延迟情况下最大的下载延迟(单位秒)。\n\n\nAUTOTHROTTLE_DEBUG¶\n默认: False\n起用AutoThrottle调试(debug)模式，展示每个接收到的response。\n您可以通过此来查看限速参数是如何实时被调整的。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "AUTOTHROTTLE_START_DELAY", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html#autothrottle-start-delay", "content": ["\n            \n    \n  \n自动限速(AutoThrottle)扩展¶\n该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。\n\n设计目标¶\n\n更友好的对待网站，而不使用默认的下载延迟0。\n自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。\n用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。\n\n\n\n扩展是如何实现的¶\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。\n不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n限速算法¶\n算法根据以下规则调整下载延迟及并发数:\n\nspider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。\n当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。\n\n\n注解\nAutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比\nDOWNLOAD_DELAY 更低的下载延迟或者比\nCONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数\n(或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。\n\n\n\n设置¶\n下面是控制AutoThrottle扩展的设置:\n\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_MAX_DELAY\nAUTOTHROTTLE_DEBUG\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nDOWNLOAD_DELAY\n\n更多内容请参考 限速算法 。\n\nAUTOTHROTTLE_ENABLED¶\n默认: False\n启用AutoThrottle扩展。\n\n\nAUTOTHROTTLE_START_DELAY¶\n默认: 5.0\n初始下载延迟(单位:秒)。\n\n\nAUTOTHROTTLE_MAX_DELAY¶\n默认: 60.0\n在高延迟情况下最大的下载延迟(单位秒)。\n\n\nAUTOTHROTTLE_DEBUG¶\n默认: False\n起用AutoThrottle调试(debug)模式，展示每个接收到的response。\n您可以通过此来查看限速参数是如何实时被调整的。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "AUTOTHROTTLE_MAX_DELAY", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html#autothrottle-max-delay", "content": ["\n            \n    \n  \n自动限速(AutoThrottle)扩展¶\n该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。\n\n设计目标¶\n\n更友好的对待网站，而不使用默认的下载延迟0。\n自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。\n用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。\n\n\n\n扩展是如何实现的¶\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。\n不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n限速算法¶\n算法根据以下规则调整下载延迟及并发数:\n\nspider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。\n当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。\n\n\n注解\nAutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比\nDOWNLOAD_DELAY 更低的下载延迟或者比\nCONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数\n(或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。\n\n\n\n设置¶\n下面是控制AutoThrottle扩展的设置:\n\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_MAX_DELAY\nAUTOTHROTTLE_DEBUG\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nDOWNLOAD_DELAY\n\n更多内容请参考 限速算法 。\n\nAUTOTHROTTLE_ENABLED¶\n默认: False\n启用AutoThrottle扩展。\n\n\nAUTOTHROTTLE_START_DELAY¶\n默认: 5.0\n初始下载延迟(单位:秒)。\n\n\nAUTOTHROTTLE_MAX_DELAY¶\n默认: 60.0\n在高延迟情况下最大的下载延迟(单位秒)。\n\n\nAUTOTHROTTLE_DEBUG¶\n默认: False\n起用AutoThrottle调试(debug)模式，展示每个接收到的response。\n您可以通过此来查看限速参数是如何实时被调整的。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "AUTOTHROTTLE_DEBUG", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html#autothrottle-debug", "content": ["\n            \n    \n  \n自动限速(AutoThrottle)扩展¶\n该扩展能根据Scrapy服务器及您爬取的网站的负载自动限制爬取速度。\n\n设计目标¶\n\n更友好的对待网站，而不使用默认的下载延迟0。\n自动调整scrapy来优化下载速度，使得用户不用调节下载延迟及并发请求数来找到优化的值。\n用户只需指定允许的最大并发请求数，剩下的都交给扩展来完成。\n\n\n\n扩展是如何实现的¶\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。\n不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n限速算法¶\n算法根据以下规则调整下载延迟及并发数:\n\nspider永远以1并发请求数及 AUTOTHROTTLE_START_DELAY 中指定的下载延迟启动。\n当接收到回复时，下载延迟会调整到该回复的延迟与之前下载延迟之间的平均值。\n\n\n注解\nAutoThrottle扩展尊重标准Scrapy设置中的并发数及延迟。这意味着其永远不会设置一个比\nDOWNLOAD_DELAY 更低的下载延迟或者比\nCONCURRENT_REQUESTS_PER_DOMAIN 更高的并发数\n(或 CONCURRENT_REQUESTS_PER_IP ，取决于您使用哪一个)。\n\n\n\n设置¶\n下面是控制AutoThrottle扩展的设置:\n\nAUTOTHROTTLE_ENABLED\nAUTOTHROTTLE_START_DELAY\nAUTOTHROTTLE_MAX_DELAY\nAUTOTHROTTLE_DEBUG\nCONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP\nDOWNLOAD_DELAY\n\n更多内容请参考 限速算法 。\n\nAUTOTHROTTLE_ENABLED¶\n默认: False\n启用AutoThrottle扩展。\n\n\nAUTOTHROTTLE_START_DELAY¶\n默认: 5.0\n初始下载延迟(单位:秒)。\n\n\nAUTOTHROTTLE_MAX_DELAY¶\n默认: 60.0\n在高延迟情况下最大的下载延迟(单位秒)。\n\n\nAUTOTHROTTLE_DEBUG¶\n默认: False\n起用AutoThrottle调试(debug)模式，展示每个接收到的response。\n您可以通过此来查看限速参数是如何实时被调整的。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Requests and Responses", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/request-response.html", "content": ["\n            \n    \n  \nBenchmarking¶\n\n0.17 新版功能.\n\nScrapy提供了一个简单的性能测试工具。其创建了一个本地HTTP服务器，并以最大可能的速度进行爬取。\n该测试性能工具目的是测试Scrapy在您的硬件上的效率，来获得一个基本的底线用于对比。\n其使用了一个简单的spider，仅跟进链接，不做任何处理。\n运行:\nscrapy bench\n\n\n您能看到类似的输出:\n2013-05-16 13:08:46-0300 [scrapy] INFO: Scrapy 0.17.0 started (bot: scrapybot)\n2013-05-16 13:08:47-0300 [follow] INFO: Spider opened\n2013-05-16 13:08:47-0300 [follow] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:48-0300 [follow] INFO: Crawled 74 pages (at 4440 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:49-0300 [follow] INFO: Crawled 143 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:50-0300 [follow] INFO: Crawled 210 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:51-0300 [follow] INFO: Crawled 274 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:52-0300 [follow] INFO: Crawled 343 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:53-0300 [follow] INFO: Crawled 410 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:54-0300 [follow] INFO: Crawled 474 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:55-0300 [follow] INFO: Crawled 538 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:56-0300 [follow] INFO: Crawled 602 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:57-0300 [follow] INFO: Closing spider (closespider_timeout)\n2013-05-16 13:08:57-0300 [follow] INFO: Crawled 666 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)\n2013-05-16 13:08:57-0300 [follow] INFO: Dumping Scrapy stats:\n    {'downloader/request_bytes': 231508,\n     'downloader/request_count': 682,\n     'downloader/request_method_count/GET': 682,\n     'downloader/response_bytes': 1172802,\n     'downloader/response_count': 682,\n     'downloader/response_status_count/200': 682,\n     'finish_reason': 'closespider_timeout',\n     'finish_time': datetime.datetime(2013, 5, 16, 16, 8, 57, 985539),\n     'log_count/INFO': 14,\n     'request_depth_max': 34,\n     'response_received_count': 682,\n     'scheduler/dequeued': 682,\n     'scheduler/dequeued/memory': 682,\n     'scheduler/enqueued': 12767,\n     'scheduler/enqueued/memory': 12767,\n     'start_time': datetime.datetime(2013, 5, 16, 16, 8, 47, 676539)}\n2013-05-16 13:08:57-0300 [follow] INFO: Spider closed (closespider_timeout)\n\n\n这说明了您的Scrapy能以3900页面/分钟的速度爬取。注意，这是一个非常简单，仅跟进链接的spider。\n任何您所编写的spider会做更多处理，从而减慢爬取的速度。\n减慢的程度取决于spider做的处理以及其是如何被编写的。\n未来会有更多的用例会被加入到性能测试套装中，以覆盖更多常见的情景。\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "Settings", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "指定设定(Designating the settings)", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#designating-the-settings", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "获取设定值(Populating the settings)", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#populating-the-settings", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "1. 命令行选项(Command line options)", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#command-line-options", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "2. 项目设定模块(Project settings module)", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#project-settings-module", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "3. 命令默认设定(Default settings per-command)", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#default-settings-per-command", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "4. 默认全局设定(Default global settings)", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#default-global-settings", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "如何访问设定(How to access settings)", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#how-to-access-settings", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "设定名字的命名规则", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#id1", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "内置设定参考手册", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#topics-settings-ref", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "AWS_ACCESS_KEY_ID", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#aws-access-key-id", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "AWS_SECRET_ACCESS_KEY", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#aws-secret-access-key", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "BOT_NAME", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#bot-name", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "CONCURRENT_ITEMS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#concurrent-items", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "CONCURRENT_REQUESTS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#concurrent-requests", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "CONCURRENT_REQUESTS_PER_DOMAIN", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#concurrent-requests-per-domain", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "CONCURRENT_REQUESTS_PER_IP", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#concurrent-requests-per-ip", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DEFAULT_ITEM_CLASS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#default-item-class", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DEFAULT_REQUEST_HEADERS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#default-request-headers", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DEPTH_LIMIT", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#depth-limit", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DEPTH_PRIORITY", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#depth-priority", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DEPTH_STATS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#depth-stats", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DEPTH_STATS_VERBOSE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#depth-stats-verbose", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DNSCACHE_ENABLED", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#dnscache-enabled", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOADER", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#downloader", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOADER_MIDDLEWARES", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#downloader-middlewares", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOADER_MIDDLEWARES_BASE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#downloader-middlewares-base", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOADER_STATS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#downloader-stats", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOAD_DELAY", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#download-delay", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOAD_HANDLERS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#download-handlers", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOAD_HANDLERS_BASE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#download-handlers-base", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOAD_TIMEOUT", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#download-timeout", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOAD_MAXSIZE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#download-maxsize", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DOWNLOAD_WARNSIZE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#download-warnsize", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DUPEFILTER_CLASS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#dupefilter-class", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "DUPEFILTER_DEBUG", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#dupefilter-debug", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "EDITOR", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#editor", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "EXTENSIONS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#extensions", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "EXTENSIONS_BASE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#extensions-base", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "ITEM_PIPELINES", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#item-pipelines", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "ITEM_PIPELINES_BASE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#item-pipelines-base", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "LOG_ENABLED", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#log-enabled", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "LOG_ENCODING", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#log-encoding", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "LOG_FILE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#log-file", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "LOG_LEVEL", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#log-level", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "LOG_STDOUT", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#log-stdout", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "MEMDEBUG_ENABLED", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#memdebug-enabled", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "MEMDEBUG_NOTIFY", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#memdebug-notify", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "MEMUSAGE_ENABLED", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#memusage-enabled", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "MEMUSAGE_LIMIT_MB", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#memusage-limit-mb", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "MEMUSAGE_NOTIFY_MAIL", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#memusage-notify-mail", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "MEMUSAGE_REPORT", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#memusage-report", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "MEMUSAGE_WARNING_MB", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#memusage-warning-mb", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "NEWSPIDER_MODULE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#newspider-module", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "RANDOMIZE_DOWNLOAD_DELAY", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#randomize-download-delay", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "REDIRECT_MAX_TIMES", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#redirect-max-times", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "REDIRECT_MAX_METAREFRESH_DELAY", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#redirect-max-metarefresh-delay", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "REDIRECT_PRIORITY_ADJUST", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#redirect-priority-adjust", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "ROBOTSTXT_OBEY", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#robotstxt-obey", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "SCHEDULER", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#scheduler", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "SPIDER_CONTRACTS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#spider-contracts", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "SPIDER_CONTRACTS_BASE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#spider-contracts-base", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "SPIDER_MANAGER_CLASS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#spider-manager-class", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "SPIDER_MIDDLEWARES", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#spider-middlewares", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "SPIDER_MIDDLEWARES_BASE", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#spider-middlewares-base", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "SPIDER_MODULES", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#spider-modules", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "STATS_CLASS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#stats-class", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "STATS_DUMP", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#stats-dump", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "STATSMAILER_RCPTS", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#statsmailer-rcpts", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "TELNETCONSOLE_ENABLED", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#telnetconsole-enabled", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "TELNETCONSOLE_PORT", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#telnetconsole-port", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "TEMPLATES_DIR", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#templates-dir", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "URLLENGTH_LIMIT", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#urllength-limit", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]},
{"title": "USER_AGENT", "url": "https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#user-agent", "content": ["\n            \n    \n  \nSettings¶\nScrapy设定(settings)提供了定制Scrapy组件的方法。您可以控制包括核心(core)，插件(extension)，pipeline及spider组件。\n设定为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。\n设定可以通过下面介绍的多种机制进行设置。\n设定(settings)同时也是选择当前激活的Scrapy项目的方法(如果您有多个的话)。\n内置设定列表请参考 内置设定参考手册 。\n\n指定设定(Designating the settings)¶\n当您使用Scrapy时，您需要声明您所使用的设定。这可以通过使用环境变量:\nSCRAPY_SETTINGS_MODULE 来完成。\nSCRAPY_SETTINGS_MODULE 必须以Python路径语法编写, 如 myproject.settings 。\n注意，设定模块应该在 Python import search path 中。\n\n\n获取设定值(Populating the settings)¶\n设定可以通过多种方式设置，每个方式具有不同的优先级。\n下面以优先级降序的方式给出方式列表:\n\n\n命令行选项(Command line Options)(最高优先级)\n每个spider的设定\n项目设定模块(Project settings module)\n命令默认设定模块(Default settings per-command)\n全局默认设定(Default global settings) (最低优先级)\n\n\n这些设定(settings)由scrapy内部很好的进行了处理，不过您仍可以使用API调用来手动处理。\n详情请参考 设置(Settings) API.\n这些机制将在下面详细介绍。\n\n1. 命令行选项(Command line options)¶\n命令行传入的参数具有最高的优先级。\n您可以使用command line 选项 -s (或 --set) 来覆盖一个(或更多)选项。\n样例:\nscrapy crawl myspider -s LOG_FILE=scrapy.log\n\n\n\n\n2. 项目设定模块(Project settings module)¶\n项目设定模块是您Scrapy项目的标准配置文件。\n其是获取大多数设定的方法。例如:: myproject.settings 。\n\n\n3. 命令默认设定(Default settings per-command)¶\n每个 Scrapy tool 命令拥有其默认设定，并覆盖了全局默认的设定。\n这些设定在命令的类的 default_settings 属性中指定。\n\n\n4. 默认全局设定(Default global settings)¶\n全局默认设定存储在 scrapy.settings.default_settings 模块，\n并在 内置设定参考手册 部分有所记录。\n\n\n\n如何访问设定(How to access settings)¶\n设定可以通过Crawler的 scrapy.crawler.Crawler.settings\n属性进行访问。其由插件及中间件的 from_crawler 方法所传入:\nclass MyExtension(object):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        if settings['LOG_ENABLED']:\n            print \"log is enabled!\"\n\n\n另外，设定可以以字典方式进行访问。不过为了避免类型错误，\n通常更希望返回需要的格式。\n这可以通过 Settings API\n提供的方法来实现。\n\n\n设定名字的命名规则¶\n设定的名字以要配置的组件作为前缀。\n例如，一个robots.txt插件的合适设定应该为\nROBOTSTXT_ENABLED, ROBOTSTXT_OBEY, ROBOTSTXT_CACHEDIR 等等。\n\n\n内置设定参考手册¶\n这里以字母序给出了所有可用的Scrapy设定及其默认值和应用范围。\n如果给出可用范围，并绑定了特定的组件，则说明了该设定使用的地方。\n这种情况下将给出该组件的模块，通常来说是插件、中间件或pipeline。\n同时也意味着为了使设定生效，该组件必须被启用。\n\nAWS_ACCESS_KEY_ID¶\n默认: None\n连接 Amazon Web services 的AWS access key。\nS3 feed storage backend 中使用.\n\n\nAWS_SECRET_ACCESS_KEY¶\n默认: None\n连接 Amazon Web services  的AWS secret key。\nS3 feed storage backend 中使用。\n\n\nBOT_NAME¶\n默认: 'scrapybot'\nScrapy项目实现的bot的名字(也为项目名称)。\n这将用来构造默认 User-Agent，同时也用来log。\n当您使用 startproject 命令创建项目时其也被自动赋值。\n\n\nCONCURRENT_ITEMS¶\n默认: 100\nItem Processor(即 Item Pipeline)\n同时处理(每个response的)item的最大值。\n\n\nCONCURRENT_REQUESTS¶\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN¶\n默认: 8\n对单个网站进行并发请求的最大值。\n\n\nCONCURRENT_REQUESTS_PER_IP¶\n默认: 0\n对单个IP进行并发请求的最大值。如果非0，则忽略\nCONCURRENT_REQUESTS_PER_DOMAIN  设定， 使用该设定。\n也就是说，并发限制将针对IP，而不是网站。\n该设定也影响 DOWNLOAD_DELAY:\n如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。\n\n\nDEFAULT_ITEM_CLASS¶\n默认: 'scrapy.item.Item'\nthe Scrapy shell 中实例化item使用的默认类。\n\n\nDEFAULT_REQUEST_HEADERS¶\n默认:\n{\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\nScrapy HTTP Request使用的默认header。由\nDefaultHeadersMiddleware\n产生。\n\n\nDEPTH_LIMIT¶\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\n\n\nDEPTH_PRIORITY¶\n默认: 0\n整数值。用于根据深度调整request优先级。\n如果为0，则不根据深度进行优先级调整。\n\n\nDEPTH_STATS¶\n默认: True\n是否收集最大深度数据。\n\n\nDEPTH_STATS_VERBOSE¶\n默认: False\n是否收集详细的深度数据。如果启用，每个深度的请求数将会被收集在数据中。\n\n\nDNSCACHE_ENABLED¶\n默认: True\n是否启用DNS内存缓存(DNS in-memory cache)。\n\n\nDOWNLOADER¶\n默认: 'scrapy.core.downloader.Downloader'\n用于crawl的downloader.\n\n\nDOWNLOADER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请查看 激活下载器中间件 。\n\n\nDOWNLOADER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,\n}\n\n\n包含Scrapy默认启用的下载中间件的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_MIDDLEWARES 。更多内容请参考\n激活下载器中间件.\n\n\nDOWNLOADER_STATS¶\n默认: True\n是否收集下载器数据。\n\n\nDOWNLOAD_DELAY¶\n默认: 0\n下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度，\n减轻服务器压力。同时也支持小数:\nDOWNLOAD_DELAY = 0.25    # 250 ms of delay\n\n\n该设定影响(默认启用的) RANDOMIZE_DOWNLOAD_DELAY 设定。\n默认情况下，Scrapy在两个请求间不等待一个固定的值，\n而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。\n当 CONCURRENT_REQUESTS_PER_IP 非0时，延迟针对的是每个ip而不是网站。\n另外您可以通过spider的 download_delay 属性为每个spider设置该设定。\n\n\nDOWNLOAD_HANDLERS¶\n默认: {}\n保存项目中启用的下载处理器(request downloader handler)的字典。\n例子请查看 DOWNLOAD_HANDLERS_BASE 。\n\n\nDOWNLOAD_HANDLERS_BASE¶\n默认:\n{\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n}\n\n\n保存项目中默认启用的下载处理器(request downloader handler)的字典。\n永远不要在项目中修改该设定，而是修改\nDOWNLOADER_HANDLERS 。\n如果需要关闭上面的下载处理器，您必须在项目中的\nDOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。\n例如，关闭文件下载处理器:\nDOWNLOAD_HANDLERS = {\n    'file': None,\n}\n\n\n\n\nDOWNLOAD_TIMEOUT¶\n默认: 180\n下载器超时时间(单位: 秒)。\n\n注解\n该超时值可以使用 download_timeout 来对每个spider进行设置, 也可以使用\ndownload_timeout Request.meta key 来对每个请求进行设置.\nThis feature needs Twisted >= 11.1.\n\n\n\nDOWNLOAD_MAXSIZE¶\nDefault: 1073741824 (1024MB)\nThe maximum response size (in bytes) that downloader will download.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_maxsize\nspider attribute and per-request using download_maxsize\nRequest.meta key.\n\n\n\nDOWNLOAD_WARNSIZE¶\nDefault: 33554432 (32Mb)\nThe response size (in bytes) that downloader will start to warn.\nIf you want to disable it set to 0.\n\n注解\nThis size can be set per spider using download_warnsize\nspider attribute and per-request using download_warnsize\nRequest.meta key.\nThis feature needs Twisted >= 11.1.\n\n\n\nDUPEFILTER_CLASS¶\n默认: 'scrapy.dupefilter.RFPDupeFilter'\n用于检测过滤重复请求的类。\n默认的 (RFPDupeFilter) 过滤器基于\nscrapy.utils.request.request_fingerprint 函数生成的请求fingerprint(指纹)。\n如果您需要修改检测的方式，您可以继承 RFPDupeFilter\n并覆盖其 request_fingerprint 方法。\n该方法接收 Request 对象并返回其fingerprint(一个字符串)。\n\n\nDUPEFILTER_DEBUG¶\n默认: False\n默认情况下， RFPDupeFilter 只记录第一次重复的请求。\n设置 DUPEFILTER_DEBUG 为 True 将会使其记录所有重复的requests。\n\n\nEDITOR¶\n默认: depends on the environment\n执行 edit 命令编辑spider时使用的编辑器。\n其默认为 EDITOR 环境变量。如果该变量未设置，其默认为 vi (Unix系统) 或者 IDLE编辑器(Windows)。\n\n\nEXTENSIONS¶\n默认:: {}\n保存项目中启用的插件及其顺序的字典。\n\n\nEXTENSIONS_BASE¶\n默认:\n{\n    'scrapy.contrib.corestats.CoreStats': 0,\n    'scrapy.telnet.TelnetConsole': 0,\n    'scrapy.contrib.memusage.MemoryUsage': 0,\n    'scrapy.contrib.memdebug.MemoryDebugger': 0,\n    'scrapy.contrib.closespider.CloseSpider': 0,\n    'scrapy.contrib.feedexport.FeedExporter': 0,\n    'scrapy.contrib.logstats.LogStats': 0,\n    'scrapy.contrib.spiderstate.SpiderState': 0,\n    'scrapy.contrib.throttle.AutoThrottle': 0,\n}\n\n\n可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下，\n该设定包含所有稳定(stable)的内置插件。\n更多内容请参考 extensions用户手册 及\n所有可用的插件 。\n\n\nITEM_PIPELINES¶\n默认: {}\n保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。\n不过值(value)习惯设定在0-1000范围内。\n为了兼容性，ITEM_PIPELINES 支持列表，不过已经被废弃了。\n样例:\nITEM_PIPELINES = {\n    'mybot.pipelines.validate.ValidateMyItem': 300,\n    'mybot.pipelines.validate.StoreMyItem': 800,\n}\n\n\n\n\nITEM_PIPELINES_BASE¶\n默认: {}\n保存项目中默认启用的pipeline的字典。\n永远不要在项目中修改该设定，而是修改\nITEM_PIPELINES 。\n\n\nLOG_ENABLED¶\n默认: True\n是否启用logging。\n\n\nLOG_ENCODING¶\n默认: 'utf-8'\nlogging使用的编码。\n\n\nLOG_FILE¶\n默认: None\nlogging输出的文件名。如果为None，则使用标准错误输出(standard error)。\n\n\nLOG_LEVEL¶\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、\nERROR、WARNING、INFO、DEBUG。更多内容请查看 Logging 。\n\n\nLOG_STDOUT¶\n默认: False\n如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中。例如，\n执行 print 'hello' ，其将会在Scrapy log中显示。\n\n\nMEMDEBUG_ENABLED¶\n默认: False\n是否启用内存调试(memory debugging)。\n\n\nMEMDEBUG_NOTIFY¶\n默认: []\n如果该设置不为空，当启用内存调试时将会发送一份内存报告到指定的地址；否则该报告将写到log中。\n样例:\nMEMDEBUG_NOTIFY = ['user@example.com']\n\n\n\n\nMEMUSAGE_ENABLED¶\n默认: False\nScope: scrapy.contrib.memusage\n是否启用内存使用插件。当Scrapy进程占用的内存超出限制时，该插件将会关闭Scrapy进程，\n同时发送email进行通知。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_LIMIT_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在关闭Scrapy之前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不做限制。\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_NOTIFY_MAIL¶\n默认: False\nScope: scrapy.contrib.memusage\n达到内存限制时通知的email列表。\nExample:\nMEMUSAGE_NOTIFY_MAIL = ['user@example.com']\n\n\nSee 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_REPORT¶\n默认: False\nScope: scrapy.contrib.memusage\n每个spider被关闭时是否发送内存使用报告。\n查看 内存使用扩展(Memory usage extension).\n\n\nMEMUSAGE_WARNING_MB¶\n默认: 0\nScope: scrapy.contrib.memusage\n在发送警告email前所允许的最大内存数(单位: MB)(如果 MEMUSAGE_ENABLED为True)。\n如果为0，将不发送警告。\n\n\nNEWSPIDER_MODULE¶\n默认: ''\n使用 genspider 命令创建新spider的模块。\n样例:\nNEWSPIDER_MODULE = 'mybot.spiders_dev'\n\n\n\n\nRANDOMIZE_DOWNLOAD_DELAY¶\n默认: True\n如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值\n(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)。\n该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求，\n查找请求之间时间的相似性。\n随机的策略与 wget --random-wait 选项的策略相同。\n若 DOWNLOAD_DELAY 为0(默认值)，该选项将不起作用。\n\n\nREDIRECT_MAX_TIMES¶\n默认: 20\n定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。\n对某些任务我们使用Firefox默认值。\n\n\nREDIRECT_MAX_METAREFRESH_DELAY¶\n默认: 100\n有些网站使用 meta-refresh 重定向到session超时页面，\n因此我们限制自动重定向到最大延迟(秒)。\n=>有点不肯定:\n\n\nREDIRECT_PRIORITY_ADJUST¶\n默认: +2\n修改重定向请求相对于原始请求的优先级。\n负数意味着更多优先级。\n\n\nROBOTSTXT_OBEY¶\n默认: False\nScope: scrapy.contrib.downloadermiddleware.robotstxt\n如果启用，Scrapy将会尊重 robots.txt策略。更多内容请查看\nRobotsTxtMiddleware 。\n\n\nSCHEDULER¶\n默认: 'scrapy.core.scheduler.Scheduler'\n用于爬取的调度器。\n\n\nSPIDER_CONTRACTS¶\n默认:: {}\n保存项目中启用用于测试spider的scrapy contract及其顺序的字典。\n更多内容请参考 Spiders Contracts 。\n\n\nSPIDER_CONTRACTS_BASE¶\n默认:\n{\n    'scrapy.contracts.default.UrlContract' : 1,\n    'scrapy.contracts.default.ReturnsContract': 2,\n    'scrapy.contracts.default.ScrapesContract': 3,\n}\n\n\n保存项目中默认启用的scrapy contract的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_CONTRACTS 。更多内容请参考\nSpiders Contracts 。\n\n\nSPIDER_MANAGER_CLASS¶\n默认: 'scrapy.spidermanager.SpiderManager'\n用于管理spider的类。该类必须实现 SpiderManager API\n\n\nSPIDER_MIDDLEWARES¶\n默认:: {}\n保存项目中启用的下载中间件及其顺序的字典。\n更多内容请参考 激活spider中间件 。\n\n\nSPIDER_MIDDLEWARES_BASE¶\n默认:\n{\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n}\n\n\n保存项目中默认启用的spider中间件的字典。\n永远不要在项目中修改该设定，而是修改\nSPIDER_MIDDLEWARES 。更多内容请参考\n激活spider中间件.\n\n\nSPIDER_MODULES¶\n默认: []\nScrapy搜索spider的模块列表。\n样例:\nSPIDER_MODULES = ['mybot.spiders_prod', 'mybot.spiders_dev']\n\n\n\n\nSTATS_CLASS¶\n默认: 'scrapy.statscol.MemoryStatsCollector'\n收集数据的类。该类必须实现\n状态收集器(Stats Collector) API.\n\n\nSTATS_DUMP¶\n默认: True\n当spider结束时dump Scrapy状态数据 (到Scrapy log中)。\n更多内容请查看 数据收集(Stats Collection) 。\n\n\nSTATSMAILER_RCPTS¶\n默认: [] (空list)\nspider完成爬取后发送Scrapy数据。更多内容请查看\nStatsMailer 。\n\n\nTELNETCONSOLE_ENABLED¶\n默认: True\n表明 telnet 终端 (及其插件)是否启用的布尔值。\n\n\nTELNETCONSOLE_PORT¶\n默认: [6023, 6073]\ntelnet终端使用的端口范围。如果设置为 None 或 0 ，\n则使用动态分配的端口。更多内容请查看\nTelnet终端(Telnet Console) 。\n\n\nTEMPLATES_DIR¶\n默认:  scrapy模块内部的 templates\n使用 startproject 命令创建项目时查找模板的目录。\n\n\nURLLENGTH_LIMIT¶\n默认: 2083\nScope: contrib.spidermiddleware.urllength\n爬取URL的最大长度。更多关于该设定的默认值信息请查看:\nhttp://www.boutell.com/newfaq/misc/urllength.html\n\n\nUSER_AGENT¶\n默认: \"Scrapy/VERSION (+http://scrapy.org)\"\n爬取的默认User-Agent，除非被覆盖。\n\n\n\n\n\n    \n        \n            讨论\n            ¶\n        \n\n        \n    \n\n           "]}
]